{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20b885e1-e125-4ad5-b9cc-cf6ed549dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the folder path where the data files are located\n",
    "data_folder = r\"C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\CommunityMerge\\3. MERGED CSV Sections\"\n",
    "\n",
    "# List of all the file names to be merged\n",
    "file_names = ['1. merged_Community_Cover.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9f29892-450c-4e90-b9d5-2fd12da901f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for file: C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\CommunityMerge\\3. MERGED CSV Sections\\1. merged_Community_Cover.csv\n",
      "File found: C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\CommunityMerge\\3. MERGED CSV Sections\\1. merged_Community_Cover.csv\n",
      "Initial duplicates found based on keys ['CID']:\n",
      "             CID  P_Name  P_ID         D_Name  D_ID                T_Name  \\\n",
      "0    101010101.0  PUNJAB   1.0          KASUR   1.0               CHUNIAN   \n",
      "1    101010202.0  PUNJAB   1.0          KASUR   1.0               CHUNIAN   \n",
      "2    101010203.0  PUNJAB   1.0          KASUR   1.0               CHUNIAN   \n",
      "3    101020304.0  PUNJAB   1.0          KASUR   1.0               PATTOKI   \n",
      "4    102030405.0  PUNJAB   1.0        BHAKKAR   2.0               BHAKKAR   \n",
      "..           ...     ...   ...            ...   ...                   ...   \n",
      "220  108182829.0  PUNJAB   1.0  BAHAWAL NAGAR   8.0      FORTABBAS TEHSIL   \n",
      "221  106142324.0  PUNJAB   1.0          JHANG   6.0        CHINIOT TEHSIL   \n",
      "222  111284344.0  PUNJAB   1.0     FAISALABAD  11.0  TANDLIAN WALA TEHSIL   \n",
      "223  105101617.0  PUNJAB   1.0         VEHARI   5.0         MAILSI TEHSIL   \n",
      "224  216436364.0   SINDH   2.0     JACCOBABAD  16.0       KANDHKOT TALUKA   \n",
      "\n",
      "     T_ID             UC_Name  UC_ID  DOI_d  ...  Language 2  Language 3  \\\n",
      "0     1.0                 NaN    1.0   21.0  ...         NaN         NaN   \n",
      "1     1.0                 NaN    2.0   24.0  ...         2.0         NaN   \n",
      "2     1.0                 NaN    2.0    2.0  ...         2.0         NaN   \n",
      "3     2.0                 NaN    3.0    6.0  ...         2.0         NaN   \n",
      "4     3.0                 NaN    4.0   20.0  ...         3.0         7.0   \n",
      "..    ...                 ...    ...    ...  ...         ...         ...   \n",
      "220  18.0  CHAK NO 176/7-R UC   28.0    2.0  ...         2.0       -99.0   \n",
      "221  14.0       KOT SHAKIR UC   23.0   24.0  ...       -99.0         NaN   \n",
      "222  28.0  CHAK NO 490/G.B UC   43.0   15.0  ...       -99.0         NaN   \n",
      "223  10.0            FADDA UC   16.0   27.0  ...         1.0       -99.0   \n",
      "224  43.0           SAIFAL UC   63.0    3.0  ...         6.0       -99.0   \n",
      "\n",
      "     M_ID  Survey_Round  Community           FID  StartTime_hrs  \\\n",
      "0     1.0           1.0        1.0  1.010101e+09           10.0   \n",
      "1     2.0           1.0        2.0  1.010102e+09           10.0   \n",
      "2     3.0           1.0        3.0  1.010102e+09           18.0   \n",
      "3     4.0           1.0        4.0  1.010203e+09            9.0   \n",
      "4     5.0           1.0        1.0  1.020304e+09           16.0   \n",
      "..    ...           ...        ...           ...            ...   \n",
      "220   NaN           NaN        NaN           NaN            NaN   \n",
      "221   NaN           NaN        NaN           NaN            NaN   \n",
      "222   NaN           NaN        NaN           NaN            NaN   \n",
      "223   NaN           NaN        NaN           NaN            NaN   \n",
      "224   NaN           NaN        NaN           NaN            NaN   \n",
      "\n",
      "     StartTime_mins  EndTime_hrs  EndTime_mins  \n",
      "0              35.0         11.0          10.0  \n",
      "1               8.0         11.0          15.0  \n",
      "2              10.0         18.0          55.0  \n",
      "3              55.0         11.0          25.0  \n",
      "4              10.0         17.0          40.0  \n",
      "..              ...          ...           ...  \n",
      "220             NaN          NaN           NaN  \n",
      "221             NaN          NaN           NaN  \n",
      "222             NaN          NaN           NaN  \n",
      "223             NaN          NaN           NaN  \n",
      "224             NaN          NaN           NaN  \n",
      "\n",
      "[220 rows x 23 columns]\n",
      "Merged data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# for a single unique identifier\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty DataFrame to hold the merged data\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each file in the list, read it, and concatenate it to the merged DataFrame\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(data_folder, file_name)  # Create the full file path\n",
    "    print(f\"Looking for file: {file_path}\")  # Debug print\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"File found: {file_path}\")  # Debug print\n",
    "        df = pd.read_csv(file_path, dtype=str)  # Read the CSV file with all columns as strings to handle mixed data types\n",
    "\n",
    "        # Convert numeric columns to double (if possible)\n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                df[col] = pd.to_numeric(df[col], errors='raise', downcast='float')\n",
    "            except ValueError:\n",
    "                # Keep non-convertible columns as strings\n",
    "                pass\n",
    "\n",
    "        merged_df = pd.concat([merged_df, df], ignore_index=True)  # Concatenate the current file's data to the merged DataFrame\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")  # Debug print\n",
    "\n",
    "# Proceed only if merged_df is not empty\n",
    "if not merged_df.empty:\n",
    "    # Define the merge keys\n",
    "    merge_keys = ['CID']\n",
    "    # Filter out merge keys that are not in the DataFrame\n",
    "    available_merge_keys = [key for key in merge_keys if key in merged_df.columns]\n",
    "\n",
    "    # Function to handle aggregation of non-merge key columns\n",
    "    def aggregate_data(group):\n",
    "        # Initialize an empty dictionary to hold aggregated results\n",
    "        agg_dict = {}\n",
    "        for col in group.columns:\n",
    "            if col in available_merge_keys:\n",
    "                # Keep merge keys as they are\n",
    "                agg_dict[col] = group[col].iloc[0]\n",
    "            else:\n",
    "                # For numeric columns, sum the values\n",
    "                if pd.api.types.is_numeric_dtype(group[col]):\n",
    "                    agg_dict[col] = group[col].sum()\n",
    "                else:\n",
    "                    # For string columns, concatenate all values\n",
    "                    agg_dict[col] = ', '.join(group[col].dropna().astype(str).unique())\n",
    "        return pd.Series(agg_dict)\n",
    "\n",
    "    # Check for initial duplicates\n",
    "    initial_duplicates = merged_df[merged_df.duplicated(subset=available_merge_keys, keep=False)]\n",
    "    if not initial_duplicates.empty:\n",
    "        print(f\"Initial duplicates found based on keys {available_merge_keys}:\")\n",
    "        print(initial_duplicates)\n",
    "\n",
    "    # Handle rows with missing keys separately\n",
    "    complete_keys_df = merged_df.dropna(subset=available_merge_keys)\n",
    "    incomplete_keys_df = merged_df[merged_df[available_merge_keys].isnull().any(axis=1)]\n",
    "\n",
    "    # Drop rows with missing merge keys\n",
    "    merged_df.dropna(subset=available_merge_keys, inplace=True)\n",
    "\n",
    "    # Apply aggregation function to each group for rows with complete keys\n",
    "    if not merged_df.empty:\n",
    "        merged_df = merged_df.groupby(available_merge_keys).apply(aggregate_data).reset_index(drop=True)\n",
    "\n",
    "    # Check for duplicates after aggregation\n",
    "    post_aggregation_duplicates = merged_df[merged_df.duplicated(subset=available_merge_keys, keep=False)]\n",
    "    if not post_aggregation_duplicates.empty:\n",
    "        print(f\"Post-aggregation duplicates found based on keys {available_merge_keys}:\")\n",
    "        print(post_aggregation_duplicates)\n",
    "\n",
    "    # Reorder columns to have available merge keys first and then the rest alphabetically\n",
    "    remaining_columns = [col for col in merged_df.columns if col not in available_merge_keys]  # Get columns excluding available merge keys\n",
    "    # Sort the remaining columns alphabetically; put columns starting with digits at the end\n",
    "    sorted_columns = sorted(remaining_columns, key=lambda x: (x[0].isdigit(), x))\n",
    "\n",
    "    # Define the final column order with available merge keys first\n",
    "    column_order = available_merge_keys + sorted_columns\n",
    "    # Reorder the DataFrame columns according to the defined order\n",
    "    merged_df = merged_df[column_order]\n",
    "\n",
    "    # Remove columns that are entirely empty\n",
    "    merged_df.dropna(axis=1, how='all', inplace=True)\n",
    "    # Remove rows that are entirely empty\n",
    "    merged_df.dropna(axis=0, how='all', inplace=True)\n",
    "\n",
    "    # Append rows with incomplete keys to the end of the DataFrame\n",
    "    final_df = pd.concat([merged_df, incomplete_keys_df], ignore_index=True)\n",
    "\n",
    "    # Save the merged and processed data to a new CSV file\n",
    "    final_df.to_csv('1. Community_Cover_CID_Level.csv', index=False)\n",
    "    print(\"Merged data saved successfully!\")\n",
    "else:\n",
    "    print(\"No data to merge and save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66f4afb9-88ff-4058-afa9-f9a684782841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for file: C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\CommunityMerge\\3. MERGED CSV Sections\\1. merged_Community_Cover.csv\n",
      "File found: C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\CommunityMerge\\3. MERGED CSV Sections\\1. merged_Community_Cover.csv\n",
      "Initial duplicates found based on keys ['CID', 'FID']:\n",
      "             CID  P_Name  P_ID         D_Name  D_ID                T_Name  \\\n",
      "76   101010101.0  PUNJAB   1.0          KASUR   1.0               CHUNIAN   \n",
      "77   101010202.0  PUNJAB   1.0          KASUR   1.0               CHUNIAN   \n",
      "78   101010203.0  PUNJAB   1.0          KASUR   1.0               CHUNIAN   \n",
      "79   101020304.0  PUNJAB   1.0          KASUR   1.0               PATTOKI   \n",
      "80   102030405.0  PUNJAB   1.0        BHAKKAR   2.0               BHAKKAR   \n",
      "..           ...     ...   ...            ...   ...                   ...   \n",
      "220  108182829.0  PUNJAB   1.0  BAHAWAL NAGAR   8.0      FORTABBAS TEHSIL   \n",
      "221  106142324.0  PUNJAB   1.0          JHANG   6.0        CHINIOT TEHSIL   \n",
      "222  111284344.0  PUNJAB   1.0     FAISALABAD  11.0  TANDLIAN WALA TEHSIL   \n",
      "223  105101617.0  PUNJAB   1.0         VEHARI   5.0         MAILSI TEHSIL   \n",
      "224  216436364.0   SINDH   2.0     JACCOBABAD  16.0       KANDHKOT TALUKA   \n",
      "\n",
      "     T_ID             UC_Name  UC_ID  DOI_d  ...  Language 2  Language 3  \\\n",
      "76    1.0              LUNDEY    1.0   15.0  ...         NaN         NaN   \n",
      "77    1.0               MOKAL    2.0    6.0  ...         NaN         NaN   \n",
      "78    1.0               MOKAL    2.0    8.0  ...         NaN         NaN   \n",
      "79    2.0         CHAK NO 027    3.0   14.0  ...         NaN         NaN   \n",
      "80    3.0               NOTAK    4.0   27.0  ...         NaN         NaN   \n",
      "..    ...                 ...    ...    ...  ...         ...         ...   \n",
      "220  18.0  CHAK NO 176/7-R UC   28.0    2.0  ...         2.0       -99.0   \n",
      "221  14.0       KOT SHAKIR UC   23.0   24.0  ...       -99.0         NaN   \n",
      "222  28.0  CHAK NO 490/G.B UC   43.0   15.0  ...       -99.0         NaN   \n",
      "223  10.0            FADDA UC   16.0   27.0  ...         1.0       -99.0   \n",
      "224  43.0           SAIFAL UC   63.0    3.0  ...         6.0       -99.0   \n",
      "\n",
      "     M_ID  Survey_Round  Community  FID  StartTime_hrs  StartTime_mins  \\\n",
      "76    1.0           NaN        NaN  NaN            NaN             NaN   \n",
      "77    2.0           NaN        NaN  NaN            NaN             NaN   \n",
      "78    3.0           NaN        NaN  NaN            NaN             NaN   \n",
      "79    4.0           NaN        NaN  NaN            NaN             NaN   \n",
      "80    5.0           NaN        NaN  NaN            NaN             NaN   \n",
      "..    ...           ...        ...  ...            ...             ...   \n",
      "220   NaN           NaN        NaN  NaN            NaN             NaN   \n",
      "221   NaN           NaN        NaN  NaN            NaN             NaN   \n",
      "222   NaN           NaN        NaN  NaN            NaN             NaN   \n",
      "223   NaN           NaN        NaN  NaN            NaN             NaN   \n",
      "224   NaN           NaN        NaN  NaN            NaN             NaN   \n",
      "\n",
      "     EndTime_hrs  EndTime_mins  \n",
      "76           NaN           NaN  \n",
      "77           NaN           NaN  \n",
      "78           NaN           NaN  \n",
      "79           NaN           NaN  \n",
      "80           NaN           NaN  \n",
      "..           ...           ...  \n",
      "220          NaN           NaN  \n",
      "221          NaN           NaN  \n",
      "222          NaN           NaN  \n",
      "223          NaN           NaN  \n",
      "224          NaN           NaN  \n",
      "\n",
      "[140 rows x 23 columns]\n",
      "Merged data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "#CID, Price_ID\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty DataFrame to hold the merged data\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each file in the list, read it, and concatenate it to the merged DataFrame\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(data_folder, file_name)  # Create the full file path\n",
    "    print(f\"Looking for file: {file_path}\")  # Debug print\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"File found: {file_path}\")  # Debug print\n",
    "        df = pd.read_csv(file_path, dtype=str)  # Read the CSV file with all columns as strings to handle mixed data types\n",
    "\n",
    "        # Convert numeric columns to double (if possible)\n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                df[col] = pd.to_numeric(df[col], errors='raise', downcast='float')\n",
    "            except ValueError:\n",
    "                # Keep non-convertible columns as strings\n",
    "                pass\n",
    "\n",
    "        merged_df = pd.concat([merged_df, df], ignore_index=True)  # Concatenate the current file's data to the merged DataFrame\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")  # Debug print\n",
    "\n",
    "# Proceed only if merged_df is not empty\n",
    "if not merged_df.empty:\n",
    "    # Define the merge keys\n",
    "    merge_keys = ['CID', 'FID']  # Example of multiple keys\n",
    "    # Filter out merge keys that are not in the DataFrame\n",
    "    available_merge_keys = [key for key in merge_keys if key in merged_df.columns]\n",
    "\n",
    "    # Function to handle aggregation of non-merge key columns\n",
    "    def aggregate_data(group):\n",
    "        # Initialize an empty dictionary to hold aggregated results\n",
    "        agg_dict = {}\n",
    "        for col in group.columns:\n",
    "            if col in available_merge_keys:\n",
    "                # Keep merge keys as they are\n",
    "                agg_dict[col] = group[col].iloc[0]\n",
    "            else:\n",
    "                # For numeric columns, sum the values\n",
    "                if pd.api.types.is_numeric_dtype(group[col]):\n",
    "                    agg_dict[col] = group[col].sum()\n",
    "                else:\n",
    "                    # For string columns, concatenate all values\n",
    "                    agg_dict[col] = ', '.join(group[col].dropna().astype(str).unique())\n",
    "        return pd.Series(agg_dict)\n",
    "\n",
    "    # Check for initial duplicates\n",
    "    initial_duplicates = merged_df[merged_df.duplicated(subset=available_merge_keys, keep=False)]\n",
    "    if not initial_duplicates.empty:\n",
    "        print(f\"Initial duplicates found based on keys {available_merge_keys}:\")\n",
    "        print(initial_duplicates)\n",
    "\n",
    "    # Handle rows with missing keys separately\n",
    "    complete_keys_df = merged_df.dropna(subset=available_merge_keys)\n",
    "    incomplete_keys_df = merged_df[merged_df[available_merge_keys].isnull().any(axis=1)]\n",
    "\n",
    "    # Drop rows with missing merge keys\n",
    "    merged_df.dropna(subset=available_merge_keys, inplace=True)\n",
    "\n",
    "    # Apply aggregation function to each group for rows with complete keys\n",
    "    if not merged_df.empty:\n",
    "        merged_df = merged_df.groupby(available_merge_keys).apply(aggregate_data).reset_index(drop=True)\n",
    "\n",
    "    # Check for duplicates after aggregation\n",
    "    post_aggregation_duplicates = merged_df[merged_df.duplicated(subset=available_merge_keys, keep=False)]\n",
    "    if not post_aggregation_duplicates.empty:\n",
    "        print(f\"Post-aggregation duplicates found based on keys {available_merge_keys}:\")\n",
    "        print(post_aggregation_duplicates)\n",
    "\n",
    "    # Reorder columns to have available merge keys first and then the rest alphabetically\n",
    "    remaining_columns = [col for col in merged_df.columns if col not in available_merge_keys]  # Get columns excluding available merge keys\n",
    "    # Sort the remaining columns alphabetically; put columns starting with digits at the end\n",
    "    sorted_columns = sorted(remaining_columns, key=lambda x: (x[0].isdigit(), x))\n",
    "\n",
    "    # Define the final column order with available merge keys first\n",
    "    column_order = available_merge_keys + sorted_columns\n",
    "    # Reorder the DataFrame columns according to the defined order\n",
    "    merged_df = merged_df[column_order]\n",
    "\n",
    "    # Remove columns that are entirely empty\n",
    "    merged_df.dropna(axis=1, how='all', inplace=True)\n",
    "    # Remove rows that are entirely empty\n",
    "    merged_df.dropna(axis=0, how='all', inplace=True)\n",
    "\n",
    "    # Append rows with incomplete keys to the end of the DataFrame\n",
    "    final_df = pd.concat([merged_df, incomplete_keys_df], ignore_index=True)\n",
    "\n",
    "    # Save the merged and processed data to a new CSV file\n",
    "    final_df.to_csv('1. Community_Cover_CID-FID_Level.csv', index=False)\n",
    "    print(\"Merged data saved successfully!\")\n",
    "else:\n",
    "    print(\"No data to merge and save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3082e06-14b5-4ada-b379-645680cbe54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for file: C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\PriceMerge\\2. MERGED CSV Sections\\2. merged_Prices_Basic-Information-about-District, UC and M-Level-Respondents.csv\n",
      "File found: C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\PriceMerge\\2. MERGED CSV Sections\\2. merged_Prices_Basic-Information-about-District, UC and M-Level-Respondents.csv\n",
      "Initial duplicates found based on keys ['CID', 'DLR_Caste', 'UCLR_Caste']:\n",
      "              CID  DLR_Caste   DLR_DOI  DLR_DOI_d  DLR_DOI_m  DLR_DOI_y  \\\n",
      "2     101010101.0      -88.0  5/5/2013       23.0        6.0     2014.0   \n",
      "3     101010101.0      -88.0  5/5/2013       23.0        6.0     2014.0   \n",
      "10    101010203.0       87.0  5/5/2013       27.0        6.0     2014.0   \n",
      "11    101010203.0       87.0  5/5/2013       27.0        6.0     2014.0   \n",
      "14    101020304.0      -88.0  5/5/2013       31.0        5.0     2014.0   \n",
      "...           ...        ...       ...        ...        ...        ...   \n",
      "1268  102040708.0        NaN       NaN        NaN        NaN        NaN   \n",
      "1269  102040708.0        NaN       NaN        NaN        NaN        NaN   \n",
      "1270  102040708.0        NaN       NaN        NaN        NaN        NaN   \n",
      "1271  109223233.0        NaN       NaN        NaN        NaN        NaN   \n",
      "1272  107152425.0        NaN       NaN        NaN        NaN        NaN   \n",
      "\n",
      "     DLR_ETI  DLR_ETI_hrs  DLR_ETI_min DLR_STI  ...  UCLR_DOI_d  UCLR_DOI_m  \\\n",
      "2      12:30         10.0         11.0   10:30  ...         NaN         NaN   \n",
      "3      12:30         10.0         57.0   10:30  ...         NaN         NaN   \n",
      "10     12:30         10.0          8.0   10:30  ...         NaN         NaN   \n",
      "11     12:30         10.0         49.0   10:30  ...         NaN         NaN   \n",
      "14     12:30         11.0         36.0   10:30  ...         NaN         NaN   \n",
      "...      ...          ...          ...     ...  ...         ...         ...   \n",
      "1268     NaN          NaN          NaN     NaN  ...         NaN         NaN   \n",
      "1269     NaN          NaN          NaN     NaN  ...         NaN         NaN   \n",
      "1270     NaN          NaN          NaN     NaN  ...         NaN         NaN   \n",
      "1271     NaN          NaN          NaN     NaN  ...         NaN         NaN   \n",
      "1272     NaN          NaN          NaN     NaN  ...         NaN         NaN   \n",
      "\n",
      "      UCLR_DOI_y  UCLR_ETI UCLR_ETI_hrs  UCLR_ETI_min  UCLR_STI  UCLR_STI_hrs  \\\n",
      "2            NaN       NaN          NaN           NaN       NaN           NaN   \n",
      "3            NaN       NaN          NaN           NaN       NaN           NaN   \n",
      "10           NaN       NaN          NaN           NaN       NaN           NaN   \n",
      "11           NaN       NaN          NaN           NaN       NaN           NaN   \n",
      "14           NaN       NaN          NaN           NaN       NaN           NaN   \n",
      "...          ...       ...          ...           ...       ...           ...   \n",
      "1268         NaN       NaN          NaN           NaN       NaN           NaN   \n",
      "1269         NaN       NaN          NaN           NaN       NaN           NaN   \n",
      "1270         NaN       NaN          NaN           NaN       NaN           NaN   \n",
      "1271         NaN       NaN          NaN           NaN       NaN           NaN   \n",
      "1272         NaN       NaN          NaN           NaN       NaN           NaN   \n",
      "\n",
      "     UCLR_STI_min  UCLR_Shop_Type  \n",
      "2             NaN             NaN  \n",
      "3             NaN             NaN  \n",
      "10            NaN             NaN  \n",
      "11            NaN             NaN  \n",
      "14            NaN             NaN  \n",
      "...           ...             ...  \n",
      "1268          NaN             NaN  \n",
      "1269          NaN             NaN  \n",
      "1270          NaN             NaN  \n",
      "1271          NaN             NaN  \n",
      "1272          NaN             NaN  \n",
      "\n",
      "[974 rows x 38 columns]\n",
      "Merged data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "#CID, Price_ID\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty DataFrame to hold the merged data\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each file in the list, read it, and concatenate it to the merged DataFrame\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(data_folder, file_name)  # Create the full file path\n",
    "    print(f\"Looking for file: {file_path}\")  # Debug print\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"File found: {file_path}\")  # Debug print\n",
    "        df = pd.read_csv(file_path, dtype=str)  # Read the CSV file with all columns as strings to handle mixed data types\n",
    "\n",
    "        # Convert numeric columns to double (if possible)\n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                df[col] = pd.to_numeric(df[col], errors='raise', downcast='float')\n",
    "            except ValueError:\n",
    "                # Keep non-convertible columns as strings\n",
    "                pass\n",
    "\n",
    "        merged_df = pd.concat([merged_df, df], ignore_index=True)  # Concatenate the current file's data to the merged DataFrame\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")  # Debug print\n",
    "\n",
    "# Proceed only if merged_df is not empty\n",
    "if not merged_df.empty:\n",
    "    # Define the merge keys\n",
    "    merge_keys = ['CID', 'DLR_Caste', 'UCLR_Caste']  # Example of multiple keys\n",
    "    # Filter out merge keys that are not in the DataFrame\n",
    "    available_merge_keys = [key for key in merge_keys if key in merged_df.columns]\n",
    "\n",
    "    # Function to handle aggregation of non-merge key columns\n",
    "    def aggregate_data(group):\n",
    "        # Initialize an empty dictionary to hold aggregated results\n",
    "        agg_dict = {}\n",
    "        for col in group.columns:\n",
    "            if col in available_merge_keys:\n",
    "                # Keep merge keys as they are\n",
    "                agg_dict[col] = group[col].iloc[0]\n",
    "            else:\n",
    "                # For numeric columns, sum the values\n",
    "                if pd.api.types.is_numeric_dtype(group[col]):\n",
    "                    agg_dict[col] = group[col].sum()\n",
    "                else:\n",
    "                    # For string columns, concatenate all values\n",
    "                    agg_dict[col] = ', '.join(group[col].dropna().astype(str).unique())\n",
    "        return pd.Series(agg_dict)\n",
    "\n",
    "    # Check for initial duplicates\n",
    "    initial_duplicates = merged_df[merged_df.duplicated(subset=available_merge_keys, keep=False)]\n",
    "    if not initial_duplicates.empty:\n",
    "        print(f\"Initial duplicates found based on keys {available_merge_keys}:\")\n",
    "        print(initial_duplicates)\n",
    "\n",
    "    # Handle rows with missing keys separately\n",
    "    complete_keys_df = merged_df.dropna(subset=available_merge_keys)\n",
    "    incomplete_keys_df = merged_df[merged_df[available_merge_keys].isnull().any(axis=1)]\n",
    "\n",
    "    # Drop rows with missing merge keys\n",
    "    merged_df.dropna(subset=available_merge_keys, inplace=True)\n",
    "\n",
    "    # Apply aggregation function to each group for rows with complete keys\n",
    "    if not merged_df.empty:\n",
    "        merged_df = merged_df.groupby(available_merge_keys).apply(aggregate_data).reset_index(drop=True)\n",
    "\n",
    "    # Check for duplicates after aggregation\n",
    "    post_aggregation_duplicates = merged_df[merged_df.duplicated(subset=available_merge_keys, keep=False)]\n",
    "    if not post_aggregation_duplicates.empty:\n",
    "        print(f\"Post-aggregation duplicates found based on keys {available_merge_keys}:\")\n",
    "        print(post_aggregation_duplicates)\n",
    "\n",
    "    # Reorder columns to have available merge keys first and then the rest alphabetically\n",
    "    remaining_columns = [col for col in merged_df.columns if col not in available_merge_keys]  # Get columns excluding available merge keys\n",
    "    # Sort the remaining columns alphabetically; put columns starting with digits at the end\n",
    "    sorted_columns = sorted(remaining_columns, key=lambda x: (x[0].isdigit(), x))\n",
    "\n",
    "    # Define the final column order with available merge keys first\n",
    "    column_order = available_merge_keys + sorted_columns\n",
    "    # Reorder the DataFrame columns according to the defined order\n",
    "    merged_df = merged_df[column_order]\n",
    "\n",
    "    # Remove columns that are entirely empty\n",
    "    merged_df.dropna(axis=1, how='all', inplace=True)\n",
    "    # Remove rows that are entirely empty\n",
    "    merged_df.dropna(axis=0, how='all', inplace=True)\n",
    "\n",
    "    # Append rows with incomplete keys to the end of the DataFrame\n",
    "    final_df = pd.concat([merged_df, incomplete_keys_df], ignore_index=True)\n",
    "\n",
    "    # Save the merged and processed data to a new CSV file\n",
    "    final_df.to_csv('2. merged_Prices_Basic-Information-about-Respondents_CID-DLR_Caste-UCLR_Caste-Levels.csv', index=False)\n",
    "    print(\"Merged data saved successfully!\")\n",
    "else:\n",
    "    print(\"No data to merge and save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f5e316-8125-4614-9152-9254cfe0f57b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
