{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e877b5-ee61-4e99-a33b-dcb46ee3c1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This notebook contains the code to merge all male data across the years based on sections.\n",
    "There are datasets for 2012, 2012-1.5, 2013 and 2014\n",
    "The different sections that will be merged are as follows:\n",
    "**2012**\n",
    "1. Cover\n",
    "2. Roaster\n",
    "3. Section 1: Education (All men 18 and above)\n",
    "4. Section 2: Agriculture\n",
    "5. Section 3: Assets\n",
    "6. Section 4: Consumption and Expenditure\n",
    "7. Section 5: Credit\n",
    "8. Section 6: Employment and Income\n",
    "9. Section 7: Economic Events/Shocks\n",
    "10. Section 8: Community Participation and Social Network Membership\n",
    "\n",
    "**2013**\n",
    "1. Cover\n",
    "2. Roaster\n",
    "3. Section 1: Education: Males 19 years and older\n",
    "4. Section 2: Agriculture\n",
    "5. Section 3: Assets\n",
    "6. Section 4: Consumption and Expenditure\n",
    "7. Section 5: Credit\n",
    "8. Section 6: Employment and Income\n",
    "9. Section 7: Health\n",
    "10. Section 8: Political Participation and Governance\n",
    "\n",
    "**2014**\n",
    "1. Cover\n",
    "2. Roaster\n",
    "3. Section 1: Education: Males 19 years and older\n",
    "4. Section 2: Agriculture\n",
    "5. Section 3: Assets\n",
    "6. Section 4: Consumption and Expenditure\n",
    "7. Section 5: Credit\n",
    "8. Section 6: Employment and Income\n",
    "9. Section 7: Economic Events/Shocks‚Äù\n",
    "10. Section 8: Participation in Social Safety Net\n",
    "11. Section 9: Siblings\n",
    "12. Section 10: Transfers\n",
    "13. Section 11: Health and Nutrition\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc85c3-5e73-405d-b1e7-4a08cad121ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "All files in the male folder will be converted to xlsx format for readability\n",
    "This will be done for all male files across the years\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Path where the .dta files are located\n",
    "# We will use just once cell for each conversion since just the path is changing\n",
    "folder_path = r'C:\\Users\\warra\\Downloads\\data\\data\\2014_data\\Male'\n",
    "\n",
    "# Get a list of all .dta files in the specified directory\n",
    "file_list = glob.glob(folder_path + '/*.dta')\n",
    "\n",
    "# Loop through the list of files\n",
    "for file in file_list:\n",
    "    # Read the .dta file into a pandas DataFrame\n",
    "    df = pd.read_stata(file, convert_categoricals=False)\n",
    "    \n",
    "    # Define the output file name by replacing .dta with .xlsx\n",
    "    output_file = file.replace('.dta', '.xlsx')\n",
    "    \n",
    "    # Write the DataFrame to an Excel file\n",
    "    df.to_excel(output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdedd007-e29c-4664-8c47-81875441e502",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following code block will merge all roaster files across the years.\n",
    "* First, we are going to read the respective files and store them as data frames\n",
    "* Next, we are going to define column mappings that I have already figured out via manual methods\n",
    "* Once the mappings are done per the set rules, we will see the new roaster dataset across the years 1.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "790eea1a-06ba-457e-bba2-c8b8d683ce5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since there are 2 files for this section in multiple years, we will make a standardized single file for sectional merging\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the first file\n",
    "df1 = pd.read_csv(r\"C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\MaleMerge\\5. Section 2 Part 3 CROP PRODUCTION, USE AND SALES IN RABI & KHARIF\\2012\\2012_s2p3a_crops & s2p3a_byprod.csv\")\n",
    "\n",
    "# Load the second file\n",
    "df2 = pd.read_excel(r\"C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\MaleMerge\\5. Section 2 Part 3 CROP PRODUCTION, USE AND SALES IN RABI & KHARIF\\2012\\2012_s2p3b_m.xlsx\")\n",
    "\n",
    "# Merge the two files based on a common column\n",
    "#merged_df = pd.merge(df1, df2, on='common_column', how='inner')\n",
    "merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Drop redundant columns\n",
    "merged_df.drop(merged_df.columns[merged_df.columns.str.contains('Unnamed', case=True)], axis=1, inplace=True)\n",
    "merged_df.drop(merged_df.columns[merged_df.columns.str.contains(' ', case=False)], axis=1, inplace=True)\n",
    "# Save the merged dataframe to a CSV file\n",
    "merged_df.to_csv('1. 2012_s2p3a_crops & s2p3a_byprod & s2p3b.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b06248f4-d50e-4ac4-9a12-91213643d91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block stores file paths to variables to make the code neat\n",
    "# The stored variables are called in the read_excel function and stored as dataframes\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Store excel file locations to variables\n",
    "agri_2012 = r\"C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\MaleMerge\\5. Section 2 Part 3 CROP PRODUCTION, USE AND SALES IN RABI & KHARIF\\2012\\1. 2012_s2p3a_crops & s2p3a_byprod & s2p3b.csv\"\n",
    "agri_2012_5 = r\"C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\MaleMerge\\5. Section 2 Part 3 CROP PRODUCTION, USE AND SALES IN RABI & KHARIF\\2012_1.5\\1. 2012_5_s1p3&s1p4.csv\"\n",
    "agri_2013 = r\"C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\MaleMerge\\5. Section 2 Part 3 CROP PRODUCTION, USE AND SALES IN RABI & KHARIF\\2013\\2013_s2p3a&s2p3b.csv\"\n",
    "agri_2014 = r\"C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\MaleMerge\\5. Section 2 Part 3 CROP PRODUCTION, USE AND SALES IN RABI & KHARIF\\2014\\2014_s2p2&s2p3.csv\"\n",
    "\n",
    "# Read excel files \n",
    "df_2012_5 = pd.read_csv(agri_2012_5)\n",
    "df_2012 = pd.read_csv(agri_2012)\n",
    "df_2013 = pd.read_csv(agri_2013)\n",
    "df_2014 = pd.read_csv(agri_2014, low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f7a81876-e34e-4771-b3fd-060be59fa3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code block will be used to standardize column names across the years to avoid discrepancies during the merging process.\n",
    "\n",
    "# Rename columns in df\n",
    "df_2012.rename(columns={\n",
    "    'hid': 'hid',\n",
    "    'round': 'round',\n",
    "    'crop_code': 's2p2_q1',\n",
    "    'crop_name': 's2p2_q2',\n",
    "    'area': 's2p2_q3',\n",
    "    'unit': 's2p2_q4',\n",
    "    's2p3aq4a': 's2p2_q5',\n",
    "    's2p3aq4b': 's2p2_q6',\n",
    "    's2p3aq5a': 's2p2_q7',\n",
    "    's2p3aq5b': 's2p2_q8',\n",
    "    's2p3aq6a': 's2p2_q9',\n",
    "    's2p3aq6b': 's2p2_q10',\n",
    "    's2p3bq1': 's2p2_q11',\n",
    "    's2p3bq2': 's2p2_q12',\n",
    "    's2p3bq3': 's2p2_q13',\n",
    "    's2p3bq4': 's2p2_q14',\n",
    "    's2p3bq5': 's2p2_q15',\n",
    "    's2p3bq6': 's2p2_q16',\n",
    "    's2p3bq7': 's2p2_q17',\n",
    "    'byprod1': 's2p2_q18',\n",
    "    'byprod1_name': 's2p2_q19',\n",
    "    'PROD1': 's2p2_q20',\n",
    "    'PROD_VAL1': 's2p2_q21',\n",
    "    'SALE1': 's2p3_q1',\n",
    "    'SAL_VAL1': 's2p3_q2',\n",
    "    'AREA_A': 's2p3_q3',\n",
    "    's2p3aq6_40kg': 's2p3_q4',\n",
    "    's2p3aq7': 's2p3_q5',\n",
    "    's2p3aq8': 's2p3_q6',\n",
    "    's2p3aq9': 's2p3_q7'\n",
    "    # Add more mappings as needed\n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "df_2012_5.rename(columns={\n",
    "    'Round': 'round',\n",
    "    'S1P3Q1': 's2p2_q1',\n",
    "    'S1P3Q2': 's2p2_q2',\n",
    "    'S1P3Q3_AREA': 's2p2_q3',\n",
    "    'S1P3Q3_UNIT': 's2p2_q4',\n",
    "    'S1P3Q4B': 's2p2_q5',\n",
    "    'S1P3Q4C': 's2p2_q6',\n",
    "    'S1P3Q4A': 's2p2_q7',\n",
    "    'S1P3Q5A': 's2p2_q8',\n",
    "    'S1P3Q5B': 's2p2_q9',\n",
    "    'S1P3Q6A': 's2p2_q10',\n",
    "    'S1P3Q6B': 's2p2_q11',\n",
    "    'S1P4Q1': 's2p2_q13',\n",
    "    'S1P4Q5': 's2p2_q14',\n",
    "    'S1P4Q2': 's2p2_q15',\n",
    "    'S1P4Q3': 's2p2_q16',\n",
    "    'S1P4Q4': 's2p2_q17',\n",
    "    'S1P4Q6': 's2p2_q18',\n",
    "    'S1P4Q7': 's2p2_q19',\n",
    "    'S1P4Q8': 's2p2_q20',\n",
    "    'S1P3Q10B': 's2p2_q21',\n",
    "    'S1P3Q10A': 's2p3_q1',\n",
    "    'S1P3Q10C': 's2p3_q2',\n",
    "    'S1P3Q10D': 's2p3_q4',\n",
    "    'S1P3Q10E': 's2p3_q5',\n",
    "    'S1P3Q10F': 's2p3_q7',\n",
    "    'S1P3Q3_LANDCONV': 's2p3_q8',\n",
    "    'S1P3Q7': 's2p3_q9',\n",
    "    'S1P3Q8': 's2p3_q10',\n",
    "    'S1P3Q9A': 's2p3_q11',\n",
    "    'S1P3Q9B': 's2p3_q9',\n",
    "    'S1P3Q11B': 'ByP_Code',\n",
    "    'S1P3Q11A': 'ByP_Name',\n",
    "    'S1P3Q11C': 'ByP_Qty_P',\n",
    "    'S1P3Q11D': 'ByP_Val_T',\n",
    "    'S1P3Q11E': 'ByP_Qty_S',\n",
    "    'S1P3_CROP_NUM': 's2p3_q11',\n",
    "    'S1P3Q9A': 'Har_Loss_Qty',\n",
    "    'S1P3Q9B': 'Har_Post_L_Res',\n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "df_2013.rename(columns={\n",
    "    'crop_code': 's2p2_q1',\n",
    "    's2p3a_q2': 's2p2_q2',\n",
    "    's2p3a_q3a': 's2p2_q3',\n",
    "    's2p3a_q3b': 's2p2_q4',\n",
    "    's2p3a_q4a': 's2p2_q5',\n",
    "    's2p3a_q4b': 's2p2_q6',\n",
    "    's2p3a_q4c': 's2p2_q7',\n",
    "    's2p3a_q5a': 's2p2_q8',\n",
    "    's2p3a_q5b': 's2p2_q9',\n",
    "    's2p3a_q6a': 's2p2_q10',\n",
    "    's2p3a_q6b': 's2p2_q11',\n",
    "    's2p3a_q6c': 's2p2_q12',\n",
    "    's2p3b_q1qty': 's2p2_q13',\n",
    "    's2p3b_q6qty': 's2p2_q14',\n",
    "    's2p3b_q3qty': 's2p2_q15',\n",
    "    's2p3b_q4qty': 's2p2_q16',\n",
    "    's2p3b_q5qty': 's2p2_q17',\n",
    "    's2p3b_q7qty': 's2p2_q18',\n",
    "    's2p3b_q8qty': 's2p2_q19',\n",
    "    's2p3b_q9': 's2p2_q20',\n",
    "    's2p3a_q10a': 's2p2_q21',\n",
    "    's2p3a_q10b': 's2p3_q1',\n",
    "    's2p3a_q10cq': 's2p3_q2',\n",
    "    's2p3a_q10cu': 's2p3_q3',\n",
    "    's2p3a_q10d': 's2p3_q4',\n",
    "    's2p3a_q10eq': 's2p3_q5',\n",
    "    's2p3a_q10eu': 's2p3_q6',\n",
    "    's2p3a_q10f': 's2p3_q7',\n",
    "    's2p3a_q3a_acres': 'Area_Plnt_Conv',\n",
    "    's2p3a_q6a_kg': 'Har_Qty_KG',\n",
    "    's2p3a_q7a': 'Har_Loss',\n",
    "    's2p3a_q7a_kg': 'Har_Loss_KG',\n",
    "    's2p3a_q7b': 'Har_Loss_T_Code',\n",
    "    's2p3a_q8': 'Har_Loss_Res',\n",
    "    's2p3a_q9q': 'Har_Loss_Qty',\n",
    "    's2p3a_q9q_kg': 'Har_Loss_code',\n",
    "    's2p3a_q9u': 'Har_Loss_U',\n",
    "    's2p3a_q9c': 'Har_Post_L_Res',\n",
    "    's2p3a_q10cq_kg': 'ByP_Qty_P_KG',\n",
    "    's2p3a_q11a': 'ByP_Code',\n",
    "    's2p3a_q11b': 'ByP_Name',\n",
    "    's2p3a_q11cq': 'ByP_Qty_P',\n",
    "    's2p3a_q11cq_kg': 'ByP_Qty_P_KG',\n",
    "    's2p3a_q11cu': 'ByP_Qty_P_U',\n",
    "    's2p3a_q11d': 'ByP_Val_T',\n",
    "    's2p3a_q11eq': 'ByP_Qty_S',\n",
    "    's2p3a_q11eq_kg': 'ByP_Qty_S_KG',\n",
    "    's2p3a_q11eu': 'ByP_Qty_S_U',\n",
    "    's2p3a_q11f': 'ByP_Val_S',\n",
    "    's2p3b_q1qty_kg': 'Qty_HH_KG',\n",
    "    's2p3b_q1u': 'Qty_HH_U',\n",
    "    's2p3b_q2qty': 'Qty_S',\n",
    "    's2p3b_q2qty_kg': 'Qty_S_KG',\n",
    "    's2p3b_q2u': 'Qty_S_U',\n",
    "    's2p3b_q3qty_kg': 'Qty_Lvs_KG',\n",
    "    's2p3b_q3u': 'Qty_Lvs_U',\n",
    "    's2p3b_q4qty_kg': 'Qty_L_KG',\n",
    "    's2p3b_q4u': 'Qty_L_U',\n",
    "    's2p3b_q5qty_kg': 'Qty_LL_KG',\n",
    "    's2p3b_q5u': 'Qty_LL_U',\n",
    "    's2p3b_q6qty_kg': 'Qty_Seed_KG',\n",
    "    's2p3b_q6u': 'Qty_Seed_U',\n",
    "    's2p3b_q7qty_kg': 'Qty_Gft_KG',\n",
    "    's2p3b_q7u': 'Qty_Gft_U',\n",
    "    's2p3b_q8qty_kg': 'Qty_Total_KG',\n",
    "    's2p3b_q8u': 'Qty_Total_U',\n",
    "    's2p3b_q10': 'Crop_Bmax',\n",
    "    's2p3b_q11': 'Crop_S_Loc'\n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "# df_2014 doesn't need renaming as it is the reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8ce41949-4ca3-48f4-abc7-a8adf6ad6fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the column mappings based on the provided positions\n",
    "# Make dictionaries for each year with updated names\n",
    "# replace with 'None' where there are no columns\n",
    "#Here are the updated mapping lists for the given datasets:\n",
    "\n",
    "\n",
    "mapping_2012 = [\n",
    "    'hid', 'round', 's2p2_q1', 's2p2_q2', 's2p2_q3', 's2p2_q4', 's2p2_q5', 's2p2_q6', 's2p2_q7', 's2p2_q8', 's2p2_q9',\n",
    "    's2p2_q10', 's2p2_q11', 's2p2_q12', 's2p2_q13', 's2p2_q14', 's2p2_q15', 's2p2_q16', 's2p2_q17', 's2p2_q18', 's2p2_q19',\n",
    "    's2p2_q20', 's2p2_q21', 's2p3_q1', 's2p3_q2', 'no column', 's2p3_q4', 's2p3_q5', 's2p3_q6', 's2p3_q7', 's2p3_q8', 's2p3_q9',\n",
    "    's2p3_q10', 's2p3_q11', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None,\n",
    "    None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, \n",
    "    None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None,\n",
    "    None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
    "]\n",
    "\n",
    "mapping_2012_5 = [\n",
    "    'hid', 'round', 's2p2_q1', 's2p2_q2', 's2p2_q3', 's2p2_q4', 's2p2_q5', 's2p2_q6', 's2p2_q7', 's2p2_q8', 's2p2_q9',\n",
    "    's2p2_q10', 's2p2_q11', None, 's2p2_q13', 's2p2_q14', 's2p2_q15', 's2p2_q16', 's2p2_q17', 's2p2_q18', 's2p2_q19', \n",
    "    's2p2_q20', 's2p2_q21', 's2p3_q1', 's2p3_q2', 's2p3_q4', 's2p3_q5', 's2p3_q7', 's2p3_q8', 's2p3_q9', 's2p3_q10', 's2p3_q11',\n",
    "    's2p3_q9', 's2p3_q10', 's2p3_q11', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None,\n",
    "    None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None,\n",
    "    None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None,\n",
    "    None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
    "]\n",
    "\n",
    "mapping_2013 = [\n",
    "    'hid', 'round', 's2p2_q1', 's2p2_q2', 's2p2_q3', 's2p2_q4', 's2p2_q5', 's2p2_q6', 's2p2_q7', 's2p2_q8', 's2p2_q9',\n",
    "    's2p2_q10', 's2p2_q11', 's2p2_q12', 's2p2_q13', 's2p2_q14', 's2p2_q15', 's2p2_q16', 's2p2_q17', 's2p2_q18', 's2p2_q19',\n",
    "    's2p2_q20', 's2p2_q21', 's2p3_q1', 's2p3_q2', 's2p3_q3', 's2p3_q4', 's2p3_q5', 's2p3_q6', 's2p3_q7', 's2p3_q8', 's2p3_q9',\n",
    "    's2p3_q10', 's2p3_q11', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None,\n",
    "    None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None,\n",
    "    None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None,\n",
    "    None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
    "]\n",
    "\n",
    "mapping_2014 = [\n",
    "    'hid', 'round', 's2p2_q1', 's2p2_q2', 's2p2_q3', 's2p2_q4', 's2p2_q5', 's2p2_q6', 's2p2_q7', 's2p2_q8', 's2p2_q9', \n",
    "    's2p2_q10', 's2p2_q11', 's2p2_q12', 's2p2_q13', 's2p2_q14', 's2p2_q15', 's2p2_q16', 's2p2_q17', 's2p2_q18', 's2p2_q19',\n",
    "    's2p2_q20', 's2p2_q21', 's2p3_q1', 's2p3_q2', 's2p3_q3', 's2p3_q4', 's2p3_q5', 's2p3_q6', 's2p3_q7', 's2p3_q8', 's2p3_q9',\n",
    "    's2p3_q10', 's2p3_q11', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, \n",
    "    None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None,\n",
    "    None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None,\n",
    "    None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None,\n",
    "    None, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b9ff0c33-1b53-44f3-8c0a-858987312571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all possible columns in the correct order\n",
    "all_columns = []\n",
    "for col in mapping_2012:\n",
    "    if col and col not in all_columns:\n",
    "        all_columns.append(col)\n",
    "for col in mapping_2012_5:\n",
    "    if col and col not in all_columns:\n",
    "        all_columns.append(col)\n",
    "for col in mapping_2013:\n",
    "    if col and col not in all_columns:\n",
    "        all_columns.append(col)\n",
    "for col in mapping_2014:\n",
    "    if col and col not in all_columns:\n",
    "        all_columns.append(col)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a8efcdee-5712-45fc-b87d-c896b489fb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def standardize_and_merge(dfs, mappings, ref_mapping, df_names):\n",
    "    \"\"\"\n",
    "    Standardize and merge dataframes based on reference mapping.\n",
    "\n",
    "    Parameters:\n",
    "    dfs (list of pd.DataFrame): List of dataframes to be merged.\n",
    "    mappings (list of list): List of mappings corresponding to each dataframe.\n",
    "    ref_mapping (list): Reference mapping to standardize the column names.\n",
    "    df_names (list): List of dataframe names.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The merged dataframe with standardized column names.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a dictionary to store columns from all dataframes\n",
    "    merged_data = {col: [] for col in ref_mapping if col}\n",
    "    # Track already included columns\n",
    "    included_cols = set(merged_data.keys())\n",
    "    \n",
    "    max_len = 0  # To track the maximum length of columns\n",
    "\n",
    "    # Iterate through each dataframe and its corresponding mapping\n",
    "    for df, mapping, df_name in zip(dfs, mappings, df_names):\n",
    "        for i, col in enumerate(df.columns):\n",
    "            if col in mapping:\n",
    "                ref_col = ref_mapping[mapping.index(col)]\n",
    "                if ref_col:  # Reference column is not None\n",
    "                    if ref_col in merged_data:\n",
    "                        merged_data[ref_col].extend(df[col].tolist())\n",
    "                    else:\n",
    "                        merged_data[ref_col] = df[col].tolist()\n",
    "                    max_len = max(max_len, len(merged_data[ref_col]))\n",
    "                else:\n",
    "                    # For columns in the dataframes but not in the reference mapping\n",
    "                    new_col_name = f\"{df_name}_{col}\"\n",
    "                    if new_col_name not in included_cols:\n",
    "                        merged_data[new_col_name] = df[col].tolist()\n",
    "                        included_cols.add(new_col_name)\n",
    "                        max_len = max(max_len, len(merged_data[new_col_name]))\n",
    "            else:\n",
    "                # Handle columns not present in the mapping\n",
    "                for j, ref_col in enumerate(ref_mapping):\n",
    "                    if not ref_col:\n",
    "                        new_col_name = f\"{df_name}_{col}\"\n",
    "                        if new_col_name not in included_cols:\n",
    "                            merged_data[new_col_name] = df[col].values.tolist() \n",
    "                            included_cols.add(new_col_name)\n",
    "                            max_len = max(max_len, len(merged_data[new_col_name]))\n",
    "\n",
    "    # Ensure all columns have the same length\n",
    "    for key in merged_data:\n",
    "        col_len = len(merged_data[key])\n",
    "        if col_len < max_len:\n",
    "            merged_data[key].extend([np.nan] * (max_len - col_len))\n",
    "\n",
    "    # Convert the merged_data dictionary to a DataFrame\n",
    "    merged_df = pd.DataFrame.from_dict(merged_data)\n",
    "    \n",
    "    # Remove columns containing 'Unnamed'\n",
    "    merged_df = merged_df.loc[:, ~merged_df.columns.str.contains('Unnamed')]\n",
    "\n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b101a410-2dff-4a7f-bb14-ea098d0f4f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes and their mappings\n",
    "dfs = [df_2012, df_2012_5, df_2013, df_2014]\n",
    "mappings = [mapping_2012, mapping_2012_5, mapping_2013, mapping_2014]\n",
    "ref_mapping = mapping_2014\n",
    "df_name= ['2012','2012.5', '2013', '2014']\n",
    "merged_df = standardize_and_merge(dfs, mappings, ref_mapping, df_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "51a5aad7-5898-4f02-948f-c3bcced01f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for the merged file (if needed)\n",
    "rename_mapping = {\n",
    "    'hid': 'HID',\n",
    "    'round': 'Survey_Round',\n",
    "    's2p2_q1': 'CC',\n",
    "    's2p2_q2': 'CN',\n",
    "    's2p2_q3': 'Plant_Area',\n",
    "    's2p2_q4': 'Unit',\n",
    "    's2p2_q5': 'Plant_D_Mon',\n",
    "    's2p2_q6': 'Plant_D_Code',\n",
    "    's2p2_q7': 'OnC_Year',\n",
    "    's2p2_q8': 'Har_T_Mon',\n",
    "    's2p2_q9': 'Har_T_Code',\n",
    "    's2p2_q10': 'Har_Crop_Q',\n",
    "    's2p2_q11': 'Har_Crop_U',\n",
    "    's2p2_q12': 'Har_Crop_S',\n",
    "    's2p2_q13': 'Qty_HH',\n",
    "    's2p2_q14': 'Qty_Seed',\n",
    "    's2p2_q15': 'Qty_Lvs',\n",
    "    's2p2_q16': 'Qty_L',\n",
    "    's2p2_q17': 'Qty_LL',\n",
    "    's2p2_q18': 'Qty_Gft',\n",
    "    's2p2_q19': 'Qty_Total',\n",
    "    's2p2_q20': 'Crop_PPU',\n",
    "    's2p2_q21': 'Ccrop_Val_T',\n",
    "    's2p3_q1': 'ByP_Code',\n",
    "    's2p3_q2': 'ByP_Name',\n",
    "    's2p3_q3': 'ByP_HH_PP',\n",
    "    's2p3_q4': 'ByP_Qty_P',\n",
    "    's2p3_q5': 'ByP_Qty_P_U',\n",
    "    's2p3_q6': 'ByP_Val_T',\n",
    "    's2p3_q7': 'ByP_Qty_LL',\n",
    "    's2p3_q8': 'ByP_Val_LL',\n",
    "    's2p3_q9': 'ByP_Qty_S',\n",
    "    's2p3_q10': 'ByP_Qty_S_U',\n",
    "    's2p3_q11': 'ByP_Val_S',\n",
    "    # Additional mapping for the repeating columns (no column) omitted for clarity\n",
    "}\n",
    "\n",
    "merged_df.rename(columns=rename_mapping, inplace=True)\n",
    "\n",
    "# Save the merged dataframe to a CSV file\n",
    "merged_df.to_csv('merged_Section_2_part_3.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a984fb-cef4-4d9e-9ac9-b680d6f41940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
