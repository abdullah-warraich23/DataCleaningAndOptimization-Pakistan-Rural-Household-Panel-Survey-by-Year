{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e877b5-ee61-4e99-a33b-dcb46ee3c1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This notebook contains the code to merge all male data across the years based on sections.\n",
    "There are datasets for 2012, 2012-1.5, 2013 and 2014\n",
    "The different sections that will be merged are as follows:\n",
    "**2012**\n",
    "1. Cover\n",
    "2. Roaster\n",
    "3. Section 1: Education (All men 18 and above)\n",
    "4. Section 2: Agriculture\n",
    "5. Section 3: Assets\n",
    "6. Section 4: Consumption and Expenditure\n",
    "7. Section 5: Credit\n",
    "8. Section 6: Employment and Income\n",
    "9. Section 7: Economic Events/Shocks\n",
    "10. Section 8: Community Participation and Social Network Membership\n",
    "\n",
    "**2013**\n",
    "1. Cover\n",
    "2. Roaster\n",
    "3. Section 1: Education: Males 19 years and older\n",
    "4. Section 2: Agriculture\n",
    "5. Section 3: Assets\n",
    "6. Section 4: Consumption and Expenditure\n",
    "7. Section 5: Credit\n",
    "8. Section 6: Employment and Income\n",
    "9. Section 7: Health\n",
    "10. Section 8: Political Participation and Governance\n",
    "\n",
    "**2014**\n",
    "1. Cover\n",
    "2. Roaster\n",
    "3. Section 1: Education: Males 19 years and older\n",
    "4. Section 2: Agriculture\n",
    "5. Section 3: Assets\n",
    "6. Section 4: Consumption and Expenditure\n",
    "7. Section 5: Credit\n",
    "8. Section 6: Employment and Income\n",
    "9. Section 7: Economic Events/Shocks‚Äù\n",
    "10. Section 8: Participation in Social Safety Net\n",
    "11. Section 9: Siblings\n",
    "12. Section 10: Transfers\n",
    "13. Section 11: Health and Nutrition\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc85c3-5e73-405d-b1e7-4a08cad121ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "All files in the male folder will be converted to xlsx format for readability\n",
    "This will be done for all male files across the years\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Path where the .dta files are located\n",
    "# We will use just once cell for each conversion since just the path is changing\n",
    "folder_path = r'C:\\Users\\warra\\Downloads\\data\\data\\2014_data\\Male'\n",
    "\n",
    "# Get a list of all .dta files in the specified directory\n",
    "file_list = glob.glob(folder_path + '/*.dta')\n",
    "\n",
    "# Loop through the list of files\n",
    "for file in file_list:\n",
    "    # Read the .dta file into a pandas DataFrame\n",
    "    df = pd.read_stata(file, convert_categoricals=False)\n",
    "    \n",
    "    # Define the output file name by replacing .dta with .xlsx\n",
    "    output_file = file.replace('.dta', '.xlsx')\n",
    "    \n",
    "    # Write the DataFrame to an Excel file\n",
    "    df.to_excel(output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdedd007-e29c-4664-8c47-81875441e502",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following code block will merge all roaster files across the years.\n",
    "* First, we are going to read the respective files and store them as data frames\n",
    "* Next, we are going to define column mappings that I have already figured out via manual methods\n",
    "* Once the mappings are done per the set rules, we will see the new roaster dataset across the years 1.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b06248f4-d50e-4ac4-9a12-91213643d91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block stores file paths to variables to make the code neat\n",
    "# The stored variables are called in the read_excel function and stored as dataframes\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Store excel file locations to variables (change it as per your path to file)\n",
    "agri_2012=r\"C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\MaleMerge\\4. Section 2 Part 2 PLOT CHARACTERISTICS FOR HOUSEHOLD-MANAGED PLOTS\\2012_s2p2_m.xlsx\"\n",
    "agri_2013=r\"C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\MaleMerge\\4. Section 2 Part 2 PLOT CHARACTERISTICS FOR HOUSEHOLD-MANAGED PLOTS\\2013_s2p2_m.xlsx\"\n",
    "agri_2012_5=r\"C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\MaleMerge\\4. Section 2 Part 2 PLOT CHARACTERISTICS FOR HOUSEHOLD-MANAGED PLOTS\\2012_1.5_s1p1.xlsx\"\n",
    "\n",
    "# Read excel files\n",
    "df_2012 = pd.read_excel(agri_2012)\n",
    "df_2013 = pd.read_excel(agri_2013)\n",
    "df_2012_5 = pd.read_excel(agri_2012_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7a81876-e34e-4771-b3fd-060be59fa3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code block will be used to standardize column names across the years to avoid discrepancies during the merging process.\n",
    "For example in the roaster data for 2013 rq21 and rq23 are not the same as rq21 and rq23 in 2014 data, but they have the same variable names. Hence, we decide to rename such columns beforehand\n",
    "We will add the updated name to the mapping dictionaries instead of the original names.\n",
    "\n",
    "'''\n",
    "\n",
    "# Rename columns in df\n",
    "\n",
    "df_2012_5.rename(columns={\n",
    "    'Round': 'round',\n",
    "    'PLOT_ID': 'plot_no',\n",
    "    'Q2_AREA': 'S2P2Q2_AREA',\n",
    "    'Q2_UNIT': 'S2P2Q2_UNIT',\n",
    "    'Q2_LANDCONV': 'S2P2Q2_AREA_A',\n",
    "    'Q3': 'S2P2Q3',\n",
    "    'Q4': 'S2P2Q4',\n",
    "    'Q5': 'S2P2Q5',\n",
    "    'Q7': 'S2P2Q6',\n",
    "    'Q9': 'S2P2Q7',\n",
    "    'Q10': 'S2P2Q8',\n",
    "    'Q11': 'S2P2Q9',\n",
    "    'Q12': 'S2P2Q10',\n",
    "    'Q16': 'S2P2Q12',\n",
    "    'C_PROVINCE': 'PROVINCE_ID',\n",
    "    'C_DISTRICT': 'DISTRICT_ID',\n",
    "    'C_TEHSIL': 'TEHSIL_ID',\n",
    "    'C_UC': 'UC_ID',\n",
    "    'C_MOUZA': 'MAUZA_ID',\n",
    "    'PLOT_NAME': 'Plt_Name',\n",
    "    'Q6': 'Rent_Rec',\n",
    "    'Q8': 'Plt_Type',\n",
    "    'Q13': 'Erosion',\n",
    "    'Q14': 'Plt_Slope',\n",
    "    'Q15': 'Waterlogging',\n",
    "    'Q17': 'tubewell'\n",
    "    \n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "df_2013.rename(columns={\n",
    "    's2p2_q2a': 'S2P2Q2_AREA',\n",
    "    's2p2_q2u': 'S2P2Q2_UNIT',\n",
    "    's2p2_q2a_acres': 'S2P2Q2_AREA_A',\n",
    "    'S2P2Q5': 'S2P2Q3',\n",
    "    's2p2_q6': 'S2P2Q4',\n",
    "    's2p2_q7': 'S2P2Q5',\n",
    "    's2p2_q8': 'S2P2Q6',\n",
    "    's2p2_q3a': 'Area_Cult_R',\n",
    "    's2p2_q3a_acres': 'Area_Cult_R_Acre',\n",
    "    's2p2_q3u': 'Area_Cult_Unit',\n",
    "    's2p2_q4a': 'Area_Cult_k',\n",
    "    's2p2_q4a_acres': 'Area_Cult_k_Acre',\n",
    "    's2p2_q4u': 'Area_Cult_K_Unit'\n",
    "}, inplace=True)\n",
    "# df_2014 doesn't need renaming as it is the reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ce41949-4ca3-48f4-abc7-a8adf6ad6fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the column mappings based on the provided positions\n",
    "# Make dictionaries for each year with updated names\n",
    "# replace with 'None' where there are no columns\n",
    "\n",
    "mapping_2012 = [\n",
    "    'hid', 'round', 'plot_no', 'S2P2Q2_AREA', 'S2P2Q2_UNIT', 'S2P2Q2_AREA_A', 'S2P2Q3', 'S2P2Q4', 'S2P2Q5', 'S2P2Q6', \n",
    "    'S2P2Q7', 'S2P2Q8', 'S2P2Q9', 'S2P2Q10', 'S2P2Q12', 'S2P2Q13', 'S2P2Q14', 'S2P2Q15', 'S2P2Q16', 'S2P2Q17', \n",
    "    'S2P2Q18', 'S2P2Q19', 'S2P2Q20', 'S2P2Q21', 'PROVINCE_ID', 'DISTRICT_ID', 'TEHSIL_ID', 'UC_ID', 'MAUZA_ID', \n",
    "    'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN', \n",
    "    'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN'\n",
    "]\n",
    "\n",
    "mapping_2012_5 = [\n",
    "    'hid', 'round', 'plot_no', 'S2P2Q2_AREA', 'S2P2Q2_UNIT', 'S2P2Q2_AREA_A', 'S2P2Q3', 'S2P2Q4', 'S2P2Q5', 'S2P2Q6', \n",
    "    'S2P2Q7', 'S2P2Q8', 'S2P2Q9', 'S2P2Q10', 'S2P2Q12', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN', \n",
    "    'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'PROVINCE_ID', 'DISTRICT_ID', 'TEHSIL_ID', \n",
    "    'UC_ID', 'MAUZA_ID', 'NO COLUMN', 'Plt_Name', 'Rent_Rec', 'Plt_Type', 'Erosion', 'Plt_Slope', 'Waterlogging', \n",
    "    'tubewell', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN'\n",
    "]\n",
    "\n",
    "mapping_2013 = [\n",
    "    'hid', 'round', 'plot_no', 'S2P2Q2_AREA', 'S2P2Q2_UNIT', 'S2P2Q2_AREA_A', 'S2P2Q3', 'S2P2Q4', 'S2P2Q5', 'S2P2Q6', \n",
    "    'S2P2Q7', 'S2P2Q8', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN', \n",
    "    'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN', \n",
    "    'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'NO COLUMN', 'Area_Cult_R', \n",
    "    'Area_Cult_R_Acre', 'Area_Cult_Unit', 'Area_Cult_k', 'Area_Cult_k_Acre', 'Area_Cult_K_Unit'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9ff0c33-1b53-44f3-8c0a-858987312571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all possible columns in the correct order\n",
    "all_columns = []\n",
    "for col in mapping_2012:\n",
    "    if col and col not in all_columns:\n",
    "        all_columns.append(col)\n",
    "for col in mapping_2012_5:\n",
    "    if col and col not in all_columns:\n",
    "        all_columns.append(col)\n",
    "for col in mapping_2013:\n",
    "    if col and col not in all_columns:\n",
    "        all_columns.append(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "787e0fcf-f875-4829-97c8-4a71ba2eecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def standardize_and_merge(dfs, mappings, all_columns):\n",
    "    merged_data = {col: [] for col in all_columns}\n",
    "\n",
    "    for df, mapping in zip(dfs, mappings):\n",
    "        for i, col in enumerate(mapping):\n",
    "            if col:\n",
    "                if col in df.columns:\n",
    "                    merged_data[col].extend(df[col].tolist())\n",
    "                else:\n",
    "                    merged_data[col].extend([np.nan] * len(df))\n",
    "    \n",
    "    max_len = max(len(v) for v in merged_data.values())\n",
    "    for key in merged_data:\n",
    "        col_len = len(merged_data[key])\n",
    "        if col_len < max_len:\n",
    "            merged_data[key].extend([np.nan] * (max_len - col_len))\n",
    "\n",
    "    merged_df = pd.DataFrame.from_dict(merged_data)\n",
    "    merged_df = merged_df.loc[:, ~merged_df.columns.str.contains('Unnamed')]\n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ca588b9-af97-48e7-8196-46949d4fb85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage with dataframes and mappings\n",
    "dfs = [df_2012, df_2013, df_2012_5]\n",
    "mappings = [mapping_2012, mapping_2013, mapping_2012_5]\n",
    "\n",
    "merged_df = standardize_and_merge(dfs, mappings, all_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51a5aad7-5898-4f02-948f-c3bcced01f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for the merged file (if needed)\n",
    "rename_mapping = {\n",
    "    'hid': 'HID',\n",
    "    'round': 'Survey_Round',\n",
    "    'plot_no': 'Plt_ID',\n",
    "    'S2P2Q2_AREA': 'Plt_Area',\n",
    "    'S2P2Q2_UNIT': 'Plt_Area_Unit',\n",
    "    'S2P2Q2_AREA_A': 'Plt_Area_Acres',\n",
    "    'S2P2Q3': 'Status_Tenancy',\n",
    "    'S2P2Q4': 'Price_per_Acre_today',\n",
    "    'S2P2Q5': 'Rent_Rin',\n",
    "    'S2P2Q6': 'Share_LL',\n",
    "    'S2P2Q7': 'Plt_IO_Village',\n",
    "    'S2P2Q8': 'Dist_Home',\n",
    "    'S2P2Q9': 'Soil_type',\n",
    "    'S2P2Q10': 'Soil_Fert',\n",
    "    'S2P2Q12': 'Salinity',\n",
    "    'S2P2Q13': 'Water_Primary',\n",
    "    'S2P2Q14': 'Water_Secondary',\n",
    "    'S2P2Q15': 'Plt_Loc',\n",
    "    'S2P2Q16': 'Water_Q',\n",
    "    'S2P2Q17': 'Alloc_IR_Time',\n",
    "    'S2P2Q18': 'Exc_IRR',\n",
    "    'S2P2Q19': 'Exch',\n",
    "    'S2P2Q20': 'Abiana_Paid',\n",
    "    'PROVINCE_ID': 'P_ID',\n",
    "    'DISTRICT_ID': 'D_ID',\n",
    "    'TEHSIL_ID': 'T_ID',\n",
    "    'UC_ID': 'UC_ID',\n",
    "    'MAUZA_ID': 'M_ID'\n",
    "}\n",
    "\n",
    "merged_df.rename(columns=rename_mapping, inplace=True)\n",
    "\n",
    "# Drop 's2p1_sr_n' if it's redundant\n",
    "if 's2p1_sr_n' in merged_df.columns:\n",
    "    merged_df.drop(columns=['s2p1_sr_n'], inplace=True)\n",
    "\n",
    "\n",
    "# Save the merged dataframe to a CSV file\n",
    "merged_df.to_csv('merged_Section_2_part_1.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a984fb-cef4-4d9e-9ac9-b680d6f41940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
