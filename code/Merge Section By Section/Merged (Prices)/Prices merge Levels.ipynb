{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20b885e1-e125-4ad5-b9cc-cf6ed549dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the folder path where the data files are located\n",
    "data_folder = r\"C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\PriceMerge\\2. MERGED CSV Sections\"\n",
    "\n",
    "# List of all the file names to be merged\n",
    "file_names = ['2. merged_Prices_Basic-Information-about-District, UC and M-Level-Respondents.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9f29892-450c-4e90-b9d5-2fd12da901f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for file: C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\PriceMerge\\2. MERGED CSV Sections\\1. merged_cover.csv\n",
      "File found: C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\PriceMerge\\2. MERGED CSV Sections\\1. merged_cover.csv\n",
      "Initial duplicates found based on keys ['CID']:\n",
      "             CID  Survey_Round  P_Name  P_ID    D_Name  D_ID           T_Name  \\\n",
      "0    101010101.0           2.0  PUNJAB   1.0     KASUR   1.0          CHUNIAN   \n",
      "1    101010202.0           2.0  PUNJAB   1.0     KASUR   1.0          CHUNIAN   \n",
      "2    101010203.0           2.0  PUNJAB   1.0     KASUR   1.0          CHUNIAN   \n",
      "3    101020304.0           2.0  PUNJAB   1.0     KASUR   1.0          PATTOKI   \n",
      "4    102030405.0           2.0  PUNJAB   1.0   BHAKKAR   2.0          BHAKKAR   \n",
      "..           ...           ...     ...   ...       ...   ...              ...   \n",
      "143  318467071.0           3.0     KPK   3.0  NOWSHERA  18.0  NOWSHERA TEHSIL   \n",
      "144  318467172.0           3.0     KPK   3.0  NOWSHERA  18.0  NOWSHERA TEHSIL   \n",
      "145  319477273.0           3.0     KPK   3.0  MANSEHRA  19.0  MANSEHRA TEHSIL   \n",
      "146  319477274.0           3.0     KPK   3.0  MANSEHRA  19.0  MANSEHRA TEHSIL   \n",
      "147  319477375.0           3.0     KPK   3.0  MANSEHRA  19.0  MANSEHRA TEHSIL   \n",
      "\n",
      "     T_ID          UC_Name  UC_ID  M_ID  \n",
      "0     1.0           LUNDEY    1.0   1.0  \n",
      "1     1.0            MOKAL    2.0   2.0  \n",
      "2     1.0            MOKAL    2.0   3.0  \n",
      "3     2.0      CHAK NO 027    3.0   4.0  \n",
      "4     3.0            NOTAK    4.0   5.0  \n",
      "..    ...              ...    ...   ...  \n",
      "143  46.0       GANDERI UC   70.0  71.0  \n",
      "144  46.0       AINZARI UC   71.0  72.0  \n",
      "145  47.0        BEHALI UC   72.0  73.0  \n",
      "146  47.0        BEHALI UC   72.0  74.0  \n",
      "147  47.0  CHATER PIAIN UC   73.0  75.0  \n",
      "\n",
      "[140 rows x 11 columns]\n",
      "Merged data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# for a single unique identifier\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty DataFrame to hold the merged data\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each file in the list, read it, and concatenate it to the merged DataFrame\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(data_folder, file_name)  # Create the full file path\n",
    "    print(f\"Looking for file: {file_path}\")  # Debug print\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"File found: {file_path}\")  # Debug print\n",
    "        df = pd.read_csv(file_path, dtype=str)  # Read the CSV file with all columns as strings to handle mixed data types\n",
    "\n",
    "        # Convert numeric columns to double (if possible)\n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                df[col] = pd.to_numeric(df[col], errors='raise', downcast='float')\n",
    "            except ValueError:\n",
    "                # Keep non-convertible columns as strings\n",
    "                pass\n",
    "\n",
    "        merged_df = pd.concat([merged_df, df], ignore_index=True)  # Concatenate the current file's data to the merged DataFrame\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")  # Debug print\n",
    "\n",
    "# Proceed only if merged_df is not empty\n",
    "if not merged_df.empty:\n",
    "    # Define the merge keys\n",
    "    merge_keys = ['CID']\n",
    "    # Filter out merge keys that are not in the DataFrame\n",
    "    available_merge_keys = [key for key in merge_keys if key in merged_df.columns]\n",
    "\n",
    "    # Function to handle aggregation of non-merge key columns\n",
    "    def aggregate_data(group):\n",
    "        # Initialize an empty dictionary to hold aggregated results\n",
    "        agg_dict = {}\n",
    "        for col in group.columns:\n",
    "            if col in available_merge_keys:\n",
    "                # Keep merge keys as they are\n",
    "                agg_dict[col] = group[col].iloc[0]\n",
    "            else:\n",
    "                # For numeric columns, sum the values\n",
    "                if pd.api.types.is_numeric_dtype(group[col]):\n",
    "                    agg_dict[col] = group[col].sum()\n",
    "                else:\n",
    "                    # For string columns, concatenate all values\n",
    "                    agg_dict[col] = ', '.join(group[col].dropna().astype(str).unique())\n",
    "        return pd.Series(agg_dict)\n",
    "\n",
    "    # Check for initial duplicates\n",
    "    initial_duplicates = merged_df[merged_df.duplicated(subset=available_merge_keys, keep=False)]\n",
    "    if not initial_duplicates.empty:\n",
    "        print(f\"Initial duplicates found based on keys {available_merge_keys}:\")\n",
    "        print(initial_duplicates)\n",
    "\n",
    "    # Handle rows with missing keys separately\n",
    "    complete_keys_df = merged_df.dropna(subset=available_merge_keys)\n",
    "    incomplete_keys_df = merged_df[merged_df[available_merge_keys].isnull().any(axis=1)]\n",
    "\n",
    "    # Drop rows with missing merge keys\n",
    "    merged_df.dropna(subset=available_merge_keys, inplace=True)\n",
    "\n",
    "    # Apply aggregation function to each group for rows with complete keys\n",
    "    if not merged_df.empty:\n",
    "        merged_df = merged_df.groupby(available_merge_keys).apply(aggregate_data).reset_index(drop=True)\n",
    "\n",
    "    # Check for duplicates after aggregation\n",
    "    post_aggregation_duplicates = merged_df[merged_df.duplicated(subset=available_merge_keys, keep=False)]\n",
    "    if not post_aggregation_duplicates.empty:\n",
    "        print(f\"Post-aggregation duplicates found based on keys {available_merge_keys}:\")\n",
    "        print(post_aggregation_duplicates)\n",
    "\n",
    "    # Reorder columns to have available merge keys first and then the rest alphabetically\n",
    "    remaining_columns = [col for col in merged_df.columns if col not in available_merge_keys]  # Get columns excluding available merge keys\n",
    "    # Sort the remaining columns alphabetically; put columns starting with digits at the end\n",
    "    sorted_columns = sorted(remaining_columns, key=lambda x: (x[0].isdigit(), x))\n",
    "\n",
    "    # Define the final column order with available merge keys first\n",
    "    column_order = available_merge_keys + sorted_columns\n",
    "    # Reorder the DataFrame columns according to the defined order\n",
    "    merged_df = merged_df[column_order]\n",
    "\n",
    "    # Remove columns that are entirely empty\n",
    "    merged_df.dropna(axis=1, how='all', inplace=True)\n",
    "    # Remove rows that are entirely empty\n",
    "    merged_df.dropna(axis=0, how='all', inplace=True)\n",
    "\n",
    "    # Append rows with incomplete keys to the end of the DataFrame\n",
    "    final_df = pd.concat([merged_df, incomplete_keys_df], ignore_index=True)\n",
    "\n",
    "    # Save the merged and processed data to a new CSV file\n",
    "    final_df.to_csv('1. merged_cover_CID_Level.csv', index=False)\n",
    "    print(\"Merged data saved successfully!\")\n",
    "else:\n",
    "    print(\"No data to merge and save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66f4afb9-88ff-4058-afa9-f9a684782841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for file: C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\PriceMerge\\2. MERGED CSV Sections\\3. merged_Prices_Prices-Of-Consumption-Items.csv\n",
      "File found: C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\PriceMerge\\2. MERGED CSV Sections\\3. merged_Prices_Prices-Of-Consumption-Items.csv\n",
      "Initial duplicates found based on keys ['CID', 'Price_ID']:\n",
      "               CID  Survey_Round  Item_Code     Unit  Qty_Unit  D_Prices  \\\n",
      "0      101010203.0           1.0        1.0        1       1.0      60.0   \n",
      "1      111284344.0           1.0        1.0        1       1.0      45.0   \n",
      "2      108192930.0           1.0        1.0        1       1.0      50.0   \n",
      "3      107172728.0           1.0        1.0        1       1.0      70.0   \n",
      "4      109233435.0           1.0        1.0        1       1.0      45.0   \n",
      "...            ...           ...        ...      ...       ...       ...   \n",
      "16935  318467172.0           3.0       94.0  Package       1.0     -44.0   \n",
      "16936  319477273.0           3.0       94.0  Package       1.0      10.0   \n",
      "16937  319477274.0           3.0       94.0  Package       1.0      10.0   \n",
      "16938  319477375.0           3.0       94.0  Package       1.0      10.0   \n",
      "16939  319487476.0           3.0       94.0  Package       1.0      10.0   \n",
      "\n",
      "       UC_Prices  M_Prices Comments      Price_ID  ...  P_ID  D_ID  T_ID  \\\n",
      "0           50.0      30.0      NaN  1.010102e+09  ...   1.0   1.0   1.0   \n",
      "1           40.0      36.0      NaN  1.112843e+09  ...   1.0  11.0  28.0   \n",
      "2           35.0      35.0      NaN  1.081929e+09  ...   1.0   8.0  19.0   \n",
      "3           40.0      32.0      NaN  1.071727e+09  ...   1.0   7.0  17.0   \n",
      "4           45.0      40.0      NaN  1.092334e+09  ...   1.0   9.0  23.0   \n",
      "...          ...       ...      ...           ...  ...   ...   ...   ...   \n",
      "16935       10.0      10.0      NaN           NaN  ...   NaN   NaN   NaN   \n",
      "16936       10.0      10.0      NaN           NaN  ...   NaN   NaN   NaN   \n",
      "16937       10.0     -44.0      NaN           NaN  ...   NaN   NaN   NaN   \n",
      "16938       10.0     -44.0      NaN           NaN  ...   NaN   NaN   NaN   \n",
      "16939       10.0      10.0      NaN           NaN  ...   NaN   NaN   NaN   \n",
      "\n",
      "               UC_Name  UC_ID  M_ID  Item_Name D_ToS  UC_ToS  M_ToS  \n",
      "0                MOKAL    2.0   3.0       Milk   1.0     1.0    4.0  \n",
      "1      CHAK NO 490/G.B   43.0  44.0     Butter   1.0     1.0    NaN  \n",
      "2           BARASAJWAR   29.0  30.0      Cream   1.0     1.0    4.0  \n",
      "3            MORJHANGI   27.0  28.0     Yogurt   1.0     4.0    4.0  \n",
      "4         JETHA BHUTTA   34.0  35.0       Beef   1.0     1.0    1.0  \n",
      "...                ...    ...   ...        ...   ...     ...    ...  \n",
      "16935              NaN    NaN   NaN        NaN   NaN     NaN    NaN  \n",
      "16936              NaN    NaN   NaN        NaN   NaN     NaN    NaN  \n",
      "16937              NaN    NaN   NaN        NaN   NaN     NaN    NaN  \n",
      "16938              NaN    NaN   NaN        NaN   NaN     NaN    NaN  \n",
      "16939              NaN    NaN   NaN        NaN   NaN     NaN    NaN  \n",
      "\n",
      "[16940 rows x 24 columns]\n",
      "Merged data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "#CID, Price_ID\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty DataFrame to hold the merged data\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each file in the list, read it, and concatenate it to the merged DataFrame\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(data_folder, file_name)  # Create the full file path\n",
    "    print(f\"Looking for file: {file_path}\")  # Debug print\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"File found: {file_path}\")  # Debug print\n",
    "        df = pd.read_csv(file_path, dtype=str)  # Read the CSV file with all columns as strings to handle mixed data types\n",
    "\n",
    "        # Convert numeric columns to double (if possible)\n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                df[col] = pd.to_numeric(df[col], errors='raise', downcast='float')\n",
    "            except ValueError:\n",
    "                # Keep non-convertible columns as strings\n",
    "                pass\n",
    "\n",
    "        merged_df = pd.concat([merged_df, df], ignore_index=True)  # Concatenate the current file's data to the merged DataFrame\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")  # Debug print\n",
    "\n",
    "# Proceed only if merged_df is not empty\n",
    "if not merged_df.empty:\n",
    "    # Define the merge keys\n",
    "    merge_keys = ['CID', 'Price_ID']  # Example of multiple keys\n",
    "    # Filter out merge keys that are not in the DataFrame\n",
    "    available_merge_keys = [key for key in merge_keys if key in merged_df.columns]\n",
    "\n",
    "    # Function to handle aggregation of non-merge key columns\n",
    "    def aggregate_data(group):\n",
    "        # Initialize an empty dictionary to hold aggregated results\n",
    "        agg_dict = {}\n",
    "        for col in group.columns:\n",
    "            if col in available_merge_keys:\n",
    "                # Keep merge keys as they are\n",
    "                agg_dict[col] = group[col].iloc[0]\n",
    "            else:\n",
    "                # For numeric columns, sum the values\n",
    "                if pd.api.types.is_numeric_dtype(group[col]):\n",
    "                    agg_dict[col] = group[col].sum()\n",
    "                else:\n",
    "                    # For string columns, concatenate all values\n",
    "                    agg_dict[col] = ', '.join(group[col].dropna().astype(str).unique())\n",
    "        return pd.Series(agg_dict)\n",
    "\n",
    "    # Check for initial duplicates\n",
    "    initial_duplicates = merged_df[merged_df.duplicated(subset=available_merge_keys, keep=False)]\n",
    "    if not initial_duplicates.empty:\n",
    "        print(f\"Initial duplicates found based on keys {available_merge_keys}:\")\n",
    "        print(initial_duplicates)\n",
    "\n",
    "    # Handle rows with missing keys separately\n",
    "    complete_keys_df = merged_df.dropna(subset=available_merge_keys)\n",
    "    incomplete_keys_df = merged_df[merged_df[available_merge_keys].isnull().any(axis=1)]\n",
    "\n",
    "    # Drop rows with missing merge keys\n",
    "    merged_df.dropna(subset=available_merge_keys, inplace=True)\n",
    "\n",
    "    # Apply aggregation function to each group for rows with complete keys\n",
    "    if not merged_df.empty:\n",
    "        merged_df = merged_df.groupby(available_merge_keys).apply(aggregate_data).reset_index(drop=True)\n",
    "\n",
    "    # Check for duplicates after aggregation\n",
    "    post_aggregation_duplicates = merged_df[merged_df.duplicated(subset=available_merge_keys, keep=False)]\n",
    "    if not post_aggregation_duplicates.empty:\n",
    "        print(f\"Post-aggregation duplicates found based on keys {available_merge_keys}:\")\n",
    "        print(post_aggregation_duplicates)\n",
    "\n",
    "    # Reorder columns to have available merge keys first and then the rest alphabetically\n",
    "    remaining_columns = [col for col in merged_df.columns if col not in available_merge_keys]  # Get columns excluding available merge keys\n",
    "    # Sort the remaining columns alphabetically; put columns starting with digits at the end\n",
    "    sorted_columns = sorted(remaining_columns, key=lambda x: (x[0].isdigit(), x))\n",
    "\n",
    "    # Define the final column order with available merge keys first\n",
    "    column_order = available_merge_keys + sorted_columns\n",
    "    # Reorder the DataFrame columns according to the defined order\n",
    "    merged_df = merged_df[column_order]\n",
    "\n",
    "    # Remove columns that are entirely empty\n",
    "    merged_df.dropna(axis=1, how='all', inplace=True)\n",
    "    # Remove rows that are entirely empty\n",
    "    merged_df.dropna(axis=0, how='all', inplace=True)\n",
    "\n",
    "    # Append rows with incomplete keys to the end of the DataFrame\n",
    "    final_df = pd.concat([merged_df, incomplete_keys_df], ignore_index=True)\n",
    "\n",
    "    # Save the merged and processed data to a new CSV file\n",
    "    final_df.to_csv('3. merged_Prices_Prices-Of-Consumption-Items_CID-Price_ID-Levels.csv', index=False)\n",
    "    print(\"Merged data saved successfully!\")\n",
    "else:\n",
    "    print(\"No data to merge and save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3082e06-14b5-4ada-b379-645680cbe54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for file: C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\PriceMerge\\2. MERGED CSV Sections\\2. merged_Prices_Basic-Information-about-District, UC and M-Level-Respondents.csv\n",
      "File found: C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\PriceMerge\\2. MERGED CSV Sections\\2. merged_Prices_Basic-Information-about-District, UC and M-Level-Respondents.csv\n",
      "Initial duplicates found based on keys ['CID', 'DLR_Caste', 'UCLR_Caste']:\n",
      "              CID  DLR_Caste   DLR_DOI  DLR_DOI_d  DLR_DOI_m  DLR_DOI_y  \\\n",
      "2     101010101.0      -88.0  5/5/2013       23.0        6.0     2014.0   \n",
      "3     101010101.0      -88.0  5/5/2013       23.0        6.0     2014.0   \n",
      "10    101010203.0       87.0  5/5/2013       27.0        6.0     2014.0   \n",
      "11    101010203.0       87.0  5/5/2013       27.0        6.0     2014.0   \n",
      "14    101020304.0      -88.0  5/5/2013       31.0        5.0     2014.0   \n",
      "...           ...        ...       ...        ...        ...        ...   \n",
      "1268  102040708.0        NaN       NaN        NaN        NaN        NaN   \n",
      "1269  102040708.0        NaN       NaN        NaN        NaN        NaN   \n",
      "1270  102040708.0        NaN       NaN        NaN        NaN        NaN   \n",
      "1271  109223233.0        NaN       NaN        NaN        NaN        NaN   \n",
      "1272  107152425.0        NaN       NaN        NaN        NaN        NaN   \n",
      "\n",
      "     DLR_ETI  DLR_ETI_hrs  DLR_ETI_min DLR_STI  ...  UCLR_DOI_d  UCLR_DOI_m  \\\n",
      "2      12:30         10.0         11.0   10:30  ...         NaN         NaN   \n",
      "3      12:30         10.0         57.0   10:30  ...         NaN         NaN   \n",
      "10     12:30         10.0          8.0   10:30  ...         NaN         NaN   \n",
      "11     12:30         10.0         49.0   10:30  ...         NaN         NaN   \n",
      "14     12:30         11.0         36.0   10:30  ...         NaN         NaN   \n",
      "...      ...          ...          ...     ...  ...         ...         ...   \n",
      "1268     NaN          NaN          NaN     NaN  ...         NaN         NaN   \n",
      "1269     NaN          NaN          NaN     NaN  ...         NaN         NaN   \n",
      "1270     NaN          NaN          NaN     NaN  ...         NaN         NaN   \n",
      "1271     NaN          NaN          NaN     NaN  ...         NaN         NaN   \n",
      "1272     NaN          NaN          NaN     NaN  ...         NaN         NaN   \n",
      "\n",
      "      UCLR_DOI_y  UCLR_ETI UCLR_ETI_hrs  UCLR_ETI_min  UCLR_STI  UCLR_STI_hrs  \\\n",
      "2            NaN       NaN          NaN           NaN       NaN           NaN   \n",
      "3            NaN       NaN          NaN           NaN       NaN           NaN   \n",
      "10           NaN       NaN          NaN           NaN       NaN           NaN   \n",
      "11           NaN       NaN          NaN           NaN       NaN           NaN   \n",
      "14           NaN       NaN          NaN           NaN       NaN           NaN   \n",
      "...          ...       ...          ...           ...       ...           ...   \n",
      "1268         NaN       NaN          NaN           NaN       NaN           NaN   \n",
      "1269         NaN       NaN          NaN           NaN       NaN           NaN   \n",
      "1270         NaN       NaN          NaN           NaN       NaN           NaN   \n",
      "1271         NaN       NaN          NaN           NaN       NaN           NaN   \n",
      "1272         NaN       NaN          NaN           NaN       NaN           NaN   \n",
      "\n",
      "     UCLR_STI_min  UCLR_Shop_Type  \n",
      "2             NaN             NaN  \n",
      "3             NaN             NaN  \n",
      "10            NaN             NaN  \n",
      "11            NaN             NaN  \n",
      "14            NaN             NaN  \n",
      "...           ...             ...  \n",
      "1268          NaN             NaN  \n",
      "1269          NaN             NaN  \n",
      "1270          NaN             NaN  \n",
      "1271          NaN             NaN  \n",
      "1272          NaN             NaN  \n",
      "\n",
      "[974 rows x 38 columns]\n",
      "Merged data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "#CID, Price_ID\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty DataFrame to hold the merged data\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each file in the list, read it, and concatenate it to the merged DataFrame\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(data_folder, file_name)  # Create the full file path\n",
    "    print(f\"Looking for file: {file_path}\")  # Debug print\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"File found: {file_path}\")  # Debug print\n",
    "        df = pd.read_csv(file_path, dtype=str)  # Read the CSV file with all columns as strings to handle mixed data types\n",
    "\n",
    "        # Convert numeric columns to double (if possible)\n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                df[col] = pd.to_numeric(df[col], errors='raise', downcast='float')\n",
    "            except ValueError:\n",
    "                # Keep non-convertible columns as strings\n",
    "                pass\n",
    "\n",
    "        merged_df = pd.concat([merged_df, df], ignore_index=True)  # Concatenate the current file's data to the merged DataFrame\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")  # Debug print\n",
    "\n",
    "# Proceed only if merged_df is not empty\n",
    "if not merged_df.empty:\n",
    "    # Define the merge keys\n",
    "    merge_keys = ['CID', 'DLR_Caste', 'UCLR_Caste']  # Example of multiple keys\n",
    "    # Filter out merge keys that are not in the DataFrame\n",
    "    available_merge_keys = [key for key in merge_keys if key in merged_df.columns]\n",
    "\n",
    "    # Function to handle aggregation of non-merge key columns\n",
    "    def aggregate_data(group):\n",
    "        # Initialize an empty dictionary to hold aggregated results\n",
    "        agg_dict = {}\n",
    "        for col in group.columns:\n",
    "            if col in available_merge_keys:\n",
    "                # Keep merge keys as they are\n",
    "                agg_dict[col] = group[col].iloc[0]\n",
    "            else:\n",
    "                # For numeric columns, sum the values\n",
    "                if pd.api.types.is_numeric_dtype(group[col]):\n",
    "                    agg_dict[col] = group[col].sum()\n",
    "                else:\n",
    "                    # For string columns, concatenate all values\n",
    "                    agg_dict[col] = ', '.join(group[col].dropna().astype(str).unique())\n",
    "        return pd.Series(agg_dict)\n",
    "\n",
    "    # Check for initial duplicates\n",
    "    initial_duplicates = merged_df[merged_df.duplicated(subset=available_merge_keys, keep=False)]\n",
    "    if not initial_duplicates.empty:\n",
    "        print(f\"Initial duplicates found based on keys {available_merge_keys}:\")\n",
    "        print(initial_duplicates)\n",
    "\n",
    "    # Handle rows with missing keys separately\n",
    "    complete_keys_df = merged_df.dropna(subset=available_merge_keys)\n",
    "    incomplete_keys_df = merged_df[merged_df[available_merge_keys].isnull().any(axis=1)]\n",
    "\n",
    "    # Drop rows with missing merge keys\n",
    "    merged_df.dropna(subset=available_merge_keys, inplace=True)\n",
    "\n",
    "    # Apply aggregation function to each group for rows with complete keys\n",
    "    if not merged_df.empty:\n",
    "        merged_df = merged_df.groupby(available_merge_keys).apply(aggregate_data).reset_index(drop=True)\n",
    "\n",
    "    # Check for duplicates after aggregation\n",
    "    post_aggregation_duplicates = merged_df[merged_df.duplicated(subset=available_merge_keys, keep=False)]\n",
    "    if not post_aggregation_duplicates.empty:\n",
    "        print(f\"Post-aggregation duplicates found based on keys {available_merge_keys}:\")\n",
    "        print(post_aggregation_duplicates)\n",
    "\n",
    "    # Reorder columns to have available merge keys first and then the rest alphabetically\n",
    "    remaining_columns = [col for col in merged_df.columns if col not in available_merge_keys]  # Get columns excluding available merge keys\n",
    "    # Sort the remaining columns alphabetically; put columns starting with digits at the end\n",
    "    sorted_columns = sorted(remaining_columns, key=lambda x: (x[0].isdigit(), x))\n",
    "\n",
    "    # Define the final column order with available merge keys first\n",
    "    column_order = available_merge_keys + sorted_columns\n",
    "    # Reorder the DataFrame columns according to the defined order\n",
    "    merged_df = merged_df[column_order]\n",
    "\n",
    "    # Remove columns that are entirely empty\n",
    "    merged_df.dropna(axis=1, how='all', inplace=True)\n",
    "    # Remove rows that are entirely empty\n",
    "    merged_df.dropna(axis=0, how='all', inplace=True)\n",
    "\n",
    "    # Append rows with incomplete keys to the end of the DataFrame\n",
    "    final_df = pd.concat([merged_df, incomplete_keys_df], ignore_index=True)\n",
    "\n",
    "    # Save the merged and processed data to a new CSV file\n",
    "    final_df.to_csv('2. merged_Prices_Basic-Information-about-Respondents_CID-DLR_Caste-UCLR_Caste-Levels.csv', index=False)\n",
    "    print(\"Merged data saved successfully!\")\n",
    "else:\n",
    "    print(\"No data to merge and save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f5e316-8125-4614-9152-9254cfe0f57b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
