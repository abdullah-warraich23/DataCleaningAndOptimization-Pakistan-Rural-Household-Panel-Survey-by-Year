{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e877b5-ee61-4e99-a33b-dcb46ee3c1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This notebook contains the code to merge all male data across the years based on sections.\n",
    "There are datasets for 2012, 2012-1.5, 2013 and 2014\n",
    "The different sections that will be merged are as follows:\n",
    "**2012**\n",
    "1. Cover\n",
    "2. Roaster\n",
    "3. Section 1: Education (All men 18 and above)\n",
    "4. Section 2: Agriculture\n",
    "5. Section 3: Assets\n",
    "6. Section 4: Consumption and Expenditure\n",
    "7. Section 5: Credit\n",
    "8. Section 6: Employment and Income\n",
    "9. Section 7: Economic Events/Shocks\n",
    "10. Section 8: Community Participation and Social Network Membership\n",
    "\n",
    "**2013**\n",
    "1. Cover\n",
    "2. Roaster\n",
    "3. Section 1: Education: Males 19 years and older\n",
    "4. Section 2: Agriculture\n",
    "5. Section 3: Assets\n",
    "6. Section 4: Consumption and Expenditure\n",
    "7. Section 5: Credit\n",
    "8. Section 6: Employment and Income\n",
    "9. Section 7: Health\n",
    "10. Section 8: Political Participation and Governance\n",
    "\n",
    "**2014**\n",
    "1. Cover\n",
    "2. Roaster\n",
    "3. Section 1: Education: Males 19 years and older\n",
    "4. Section 2: Agriculture\n",
    "5. Section 3: Assets\n",
    "6. Section 4: Consumption and Expenditure\n",
    "7. Section 5: Credit\n",
    "8. Section 6: Employment and Income\n",
    "9. Section 7: Economic Events/Shocks‚Äù\n",
    "10. Section 8: Participation in Social Safety Net\n",
    "11. Section 9: Siblings\n",
    "12. Section 10: Transfers\n",
    "13. Section 11: Health and Nutrition\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc85c3-5e73-405d-b1e7-4a08cad121ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "All files in the male folder will be converted to xlsx format for readability\n",
    "This will be done for all male files across the years\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Path where the .dta files are located\n",
    "# We will use just once cell for each conversion since just the path is changing\n",
    "folder_path = r'C:\\Users\\warra\\Downloads\\data\\data\\2014_data\\Male'\n",
    "\n",
    "# Get a list of all .dta files in the specified directory\n",
    "file_list = glob.glob(folder_path + '/*.dta')\n",
    "\n",
    "# Loop through the list of files\n",
    "for file in file_list:\n",
    "    # Read the .dta file into a pandas DataFrame\n",
    "    df = pd.read_stata(file, convert_categoricals=False)\n",
    "    \n",
    "    # Define the output file name by replacing .dta with .xlsx\n",
    "    output_file = file.replace('.dta', '.xlsx')\n",
    "    \n",
    "    # Write the DataFrame to an Excel file\n",
    "    df.to_excel(output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdedd007-e29c-4664-8c47-81875441e502",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following code block will merge all roaster files across the years.\n",
    "* First, we are going to read the respective files and store them as data frames\n",
    "* Next, we are going to define column mappings that I have already figured out via manual methods\n",
    "* Once the mappings are done per the set rules, we will see the new roaster dataset across the years 1.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b06248f4-d50e-4ac4-9a12-91213643d91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block stores file paths to variables to make the code neat\n",
    "# The stored variables are called in the read_excel function and stored as dataframes\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Store excel file locations to variables (change it as per your path to file)\n",
    "agri_2012=r\"C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\MaleMerge\\6. Section 2 Part 4 INPUT USE IN RABI\\2012_s2p4_m.xlsx\"\n",
    "agri_2013=r\"C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\MaleMerge\\6. Section 2 Part 4 INPUT USE IN RABI\\2013_s2p4_m.xlsx\"\n",
    "agri_2014=r\"C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\MaleMerge\\6. Section 2 Part 4 INPUT USE IN RABI\\2014_s2p4_m.xlsx\"\n",
    "\n",
    "# Read excel files\n",
    "df_2012 = pd.read_excel(agri_2012)\n",
    "df_2013 = pd.read_excel(agri_2013)\n",
    "df_2014 = pd.read_excel(agri_2014)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7a81876-e34e-4771-b3fd-060be59fa3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code block will be used to standardize column names across the years to avoid discrepancies during the merging process.\n",
    "For example in the roaster data for 2013 rq21 and rq23 are not the same as rq21 and rq23 in 2014 data, but they have the same variable names. Hence, we decide to rename such columns beforehand\n",
    "We will add the updated name to the mapping dictionaries instead of the original names.\n",
    "\n",
    "'''\n",
    "\n",
    "# Rename columns in df\n",
    "\n",
    "df_2012.rename(columns={\n",
    "    'crop_code': 's2p4_q1',\n",
    "    's2p4q3a': 's2p4_q2',\n",
    "    's2p4q4a': 's2p4_q3',\n",
    "    's2p4q5a': 's2p4_q4',\n",
    "    's2p4q2a': 's2p4_q5',\n",
    "    's2p4q6a': 's2p4_q8',\n",
    "    'area': 'Area',\n",
    "    'unit': 'Unit',\n",
    "    's2p4q2b': 'Exp_Seed_Own',\n",
    "    's2p4q3b': 'Exp_Pest_Own',\n",
    "    's2p4q4b': 'Exp_Fert_Own',\n",
    "    's2p4q5b': 'Exp_Irr_Own',\n",
    "    's2p4q6b': 'Exp_Misc_Own',\n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "df_2013.rename(columns={\n",
    "    'crop_code': 's2p4_q1',\n",
    "    's2p4_q1a': 's2p4_q2',\n",
    "    's2p4_q2a': 's2p4_q3',\n",
    "    's2p4_q3a': 's2p4_q4',\n",
    "    's2p4_q4e': 's2p4_q5',\n",
    "    's2p4_q5a': 's2p4_q8',\n",
    "    's2p4_q4f': 'Exp_Seed_Own',\n",
    "    's2p4_q1b': 'Exp_Pest_Own',\n",
    "    's2p4_q2b': 'Exp_Fert_Own',\n",
    "    's2p4_q3b': 'Exp_Irr_Own',\n",
    "    's2p4_q5b': 'Exp_Misc_Own',\n",
    "    's2p4_q4a': 'Exp_Seed_T_Pq',\n",
    "    's2p4_q4a_kg': 'Exp_Seed_T_PqKG',\n",
    "    's2p4_q4b': 'Exp_Seed_T_Pu',\n",
    "    's2p4_q4c': 'Exp_Seed_T_Oq',\n",
    "    's2p4_q4c_kg': 'Exp_Seed_T_OqKG',\n",
    "    's2p4_q4d': 'Exp_Seed_T_Ou',\n",
    "}, inplace=True)\n",
    "# df_2014 doesn't need renaming as it is the reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ce41949-4ca3-48f4-abc7-a8adf6ad6fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the column mappings based on the provided positions\n",
    "# Make dictionaries for each year with updated names\n",
    "# replace with 'None' where there are no columns\n",
    "\n",
    "mapping_2012 = [\n",
    "    'hid', 'round', 's2p4_q1', 's2p4_q2', 's2p4_q3', 's2p4_q4', 's2p4_q5', None, None,\n",
    "    's2p4_q8', 'Area', 'Unit', 'Exp_Seed_Own', 'Exp_Pest_Own', 'Exp_Fert_Own', 'Exp_Irr_Own', 'Exp_Misc_Own', None, None,\n",
    "    None, None, None, None\n",
    "]\n",
    "\n",
    "mapping_2013 = [\n",
    "    'hid', 'round', 's2p4_q1', 's2p4_q2', 's2p4_q3', 's2p4_q4', 's2p4_q5', 's2p4_q8', None, None,\n",
    "    None, None, 'Exp_Seed_Own', 'Exp_Pest_Own', 'Exp_Fert_Own', 'Exp_Irr_Own', 'Exp_Misc_Own', 'Exp_Seed_T_Pq',\n",
    "    'Exp_Seed_T_PqKG', 'Exp_Seed_T_Pu', 'Exp_Seed_T_Oq', 'Exp_Seed_T_OqKG', 'Exp_Seed_T_Ou'\n",
    "]\n",
    "\n",
    "mapping_2014 = [\n",
    "    'hid', 'round', 's2p4_q1', 's2p4_q2', 's2p4_q3', 's2p4_q4', 's2p4_q5', 's2p4_q6', 's2p4_q7', 's2p4_q8',\n",
    "    None, None, None, None, None, None, None, None,\n",
    "    None, None, None, None, None\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9ff0c33-1b53-44f3-8c0a-858987312571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all possible columns in the correct order\n",
    "all_columns = []\n",
    "for col in mapping_2012:\n",
    "    if col and col not in all_columns:\n",
    "        all_columns.append(col)\n",
    "for col in mapping_2013:\n",
    "    if col and col not in all_columns:\n",
    "        all_columns.append(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "787e0fcf-f875-4829-97c8-4a71ba2eecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to standardize and merge DataFrames\n",
    "def standardize_and_merge(dfs, mappings, all_columns):\n",
    "    merged_data = {col: [] for col in all_columns}\n",
    "\n",
    "    for df, mapping in zip(dfs, mappings):\n",
    "        print(f\"Processing DataFrame with columns: {df.columns.tolist()}\")\n",
    "        for i, col in enumerate(mapping):\n",
    "            if col:\n",
    "                ref_col = col\n",
    "                if col in df.columns:\n",
    "                    print(f\"Appending data for column {col}\")\n",
    "                    merged_data[ref_col].extend(df[col].tolist())\n",
    "                else:\n",
    "                    print(f\"Column {col} not found in DataFrame. Adding NaNs.\")\n",
    "                    merged_data[ref_col].extend([np.nan] * len(df))\n",
    "    \n",
    "    max_len = max(len(v) for v in merged_data.values())\n",
    "    for key in merged_data:\n",
    "        col_len = len(merged_data[key])\n",
    "        if col_len < max_len:\n",
    "            merged_data[key].extend([np.nan] * (max_len - col_len))\n",
    "\n",
    "    merged_df = pd.DataFrame.from_dict(merged_data)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ca588b9-af97-48e7-8196-46949d4fb85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DataFrame with columns: ['Unnamed: 0', 'hid', 'round', 's2p4_q1', 'Area', 'Unit', 's2p4_q5', 'Exp_Seed_Own', 's2p4_q2', 'Exp_Pest_Own', 's2p4_q3', 'Exp_Fert_Own', 's2p4_q4', 'Exp_Irr_Own', 's2p4_q8', 'Exp_Misc_Own']\n",
      "Appending data for column hid\n",
      "Appending data for column round\n",
      "Appending data for column s2p4_q1\n",
      "Appending data for column s2p4_q2\n",
      "Appending data for column s2p4_q3\n",
      "Appending data for column s2p4_q4\n",
      "Appending data for column s2p4_q5\n",
      "Appending data for column s2p4_q8\n",
      "Appending data for column Area\n",
      "Appending data for column Unit\n",
      "Appending data for column Exp_Seed_Own\n",
      "Appending data for column Exp_Pest_Own\n",
      "Appending data for column Exp_Fert_Own\n",
      "Appending data for column Exp_Irr_Own\n",
      "Appending data for column Exp_Misc_Own\n",
      "Processing DataFrame with columns: ['Unnamed: 0', 'hid', 'round', 'crop_name', 's2p4_q1', 's2p4_q2', 'Exp_Pest_Own', 's2p4_q3', 'Exp_Fert_Own', 's2p4_q4', 'Exp_Irr_Own', 'Exp_Seed_T_Pq', 'Exp_Seed_T_PqKG', 'Exp_Seed_T_Pu', 'Exp_Seed_T_Oq', 'Exp_Seed_T_OqKG', 'Exp_Seed_T_Ou', 's2p4_q5', 'Exp_Seed_Own', 's2p4_q8', 'Exp_Misc_Own']\n",
      "Appending data for column hid\n",
      "Appending data for column round\n",
      "Appending data for column s2p4_q1\n",
      "Appending data for column s2p4_q2\n",
      "Appending data for column s2p4_q3\n",
      "Appending data for column s2p4_q4\n",
      "Appending data for column s2p4_q5\n",
      "Appending data for column s2p4_q8\n",
      "Appending data for column Exp_Seed_Own\n",
      "Appending data for column Exp_Pest_Own\n",
      "Appending data for column Exp_Fert_Own\n",
      "Appending data for column Exp_Irr_Own\n",
      "Appending data for column Exp_Misc_Own\n",
      "Appending data for column Exp_Seed_T_Pq\n",
      "Appending data for column Exp_Seed_T_PqKG\n",
      "Appending data for column Exp_Seed_T_Pu\n",
      "Appending data for column Exp_Seed_T_Oq\n",
      "Appending data for column Exp_Seed_T_OqKG\n",
      "Appending data for column Exp_Seed_T_Ou\n",
      "Processing DataFrame with columns: ['Unnamed: 0', 'hid', 'round', 's2p4_q1', 's2p4_q2', 's2p4_q3', 's2p4_q4', 's2p4_q5', 's2p4_q6', 's2p4_q7', 's2p4_q8']\n",
      "Appending data for column hid\n",
      "Appending data for column round\n",
      "Appending data for column s2p4_q1\n",
      "Appending data for column s2p4_q2\n",
      "Appending data for column s2p4_q3\n",
      "Appending data for column s2p4_q4\n",
      "Appending data for column s2p4_q5\n",
      "Appending data for column s2p4_q6\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'s2p4_q6'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m dfs \u001b[38;5;241m=\u001b[39m [df_2012, df_2013, df_2014]\n\u001b[0;32m      3\u001b[0m mappings \u001b[38;5;241m=\u001b[39m [mapping_2012, mapping_2013, mapping_2014]\n\u001b[1;32m----> 5\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m standardize_and_merge(dfs, mappings, all_columns)\n",
      "Cell \u001b[1;32mIn[16], line 12\u001b[0m, in \u001b[0;36mstandardize_and_merge\u001b[1;34m(dfs, mappings, all_columns)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAppending data for column \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m     merged_data[ref_col]\u001b[38;5;241m.\u001b[39mextend(df[col]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in DataFrame. Adding NaNs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 's2p4_q6'"
     ]
    }
   ],
   "source": [
    "# Usage with dataframes and mappings\n",
    "dfs = [df_2012, df_2013, df_2014]\n",
    "mappings = [mapping_2012, mapping_2013, mapping_2014]\n",
    "\n",
    "merged_df = standardize_and_merge(dfs, mappings, all_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51a5aad7-5898-4f02-948f-c3bcced01f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for the merged file (if needed)\n",
    "rename_mapping = {\n",
    "        'hid': 'HID',\n",
    "        'round': 'Survey_Round',\n",
    "        's2p4_q1': 'CC',\n",
    "        's2p4_q2': 'Exp_Pest_T',\n",
    "        's2p4_q3': 'Exp_Fert_T',\n",
    "        's2p4_q4': 'Exp_Irr_T',\n",
    "        's2p4_q5': 'Exp_Seed_T',\n",
    "        's2p4_q6': 'Exp_Lab',\n",
    "        's2p4_q7': 'Exp_Mech',\n",
    "        's2p4_q8': 'Exp_Misc',\n",
    "        # Add other renamings here\n",
    "    }\n",
    "\n",
    "merged_df.rename(columns=rename_mapping, inplace=True)\n",
    "\n",
    "# Save the merged dataframe to a CSV file\n",
    "merged_df.to_csv('merged_Section_2_part_4.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a984fb-cef4-4d9e-9ac9-b680d6f41940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
