{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86754fdb-b6f7-4567-83a9-871a433ff383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder path where the data files are located\n",
    "data_folder = r\"C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\MaleMerge\\3. MERGED CSV Sections\"\n",
    "\n",
    "# List of all the file names to be merged\n",
    "file_names = ['3. Employment&Income_Household-HouseholdMember_Level.csv']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20b885e1-e125-4ad5-b9cc-cf6ed549dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the folder path where the data files are located\n",
    "data_folder = r\"C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\MaleMerge\\3. MERGED CSV Sections\"\n",
    "\n",
    "# List of all the file names to be merged\n",
    "file_names = ['17. ClimateChange_Household-HouseholdMember_Level.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9f29892-450c-4e90-b9d5-2fd12da901f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for file: C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\MaleMerge\\3. MERGED CSV Sections\\17. ClimateChange_Household-HouseholdMember_Level.csv\n",
      "File found: C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\MaleMerge\\3. MERGED CSV Sections\\17. ClimateChange_Household-HouseholdMember_Level.csv\n",
      "Initial duplicates found based on keys ['HID']:\n",
      "               HID  C_HH_NUM  D_ID  M_ID  PCC_Atti  PCC_CiAR  PCC_CiT  \\\n",
      "0     1.010101e+10       4.0   1.0   1.0       3.0       2.0      1.0   \n",
      "1     1.010101e+10       8.0   1.0   1.0       1.0       2.0      1.0   \n",
      "2     1.010101e+10      15.0   1.0   1.0       0.0       2.0      1.0   \n",
      "3     1.010101e+10      19.0   1.0   1.0       4.0       2.0      1.0   \n",
      "4     1.010101e+10      23.0   1.0   1.0       1.0       2.0      1.0   \n",
      "...            ...       ...   ...   ...       ...       ...      ...   \n",
      "1879  3.194774e+10      22.0  19.0  75.0       NaN       NaN      NaN   \n",
      "1880  3.194774e+10      24.0  19.0  75.0       NaN       NaN      NaN   \n",
      "1881  3.194774e+10      25.0  19.0  75.0       NaN       NaN      NaN   \n",
      "1882  3.194875e+10      12.0  19.0  76.0       NaN       NaN      NaN   \n",
      "1883  3.194875e+10      22.0  19.0  76.0       NaN       NaN      NaN   \n",
      "\n",
      "      PCC_Concern  PCC_Concern_Res1  PCC_Concern_Res2  ...  RCC_RMS_MixCLP  \\\n",
      "0             3.0               1.0               7.0  ...             NaN   \n",
      "1             3.0               1.0               2.0  ...             NaN   \n",
      "2             0.0               NaN               NaN  ...             NaN   \n",
      "3             3.0               1.0               2.0  ...             NaN   \n",
      "4             3.0               1.0               2.0  ...             NaN   \n",
      "...           ...               ...               ...  ...             ...   \n",
      "1879          NaN               NaN               NaN  ...             NaN   \n",
      "1880          NaN               NaN               NaN  ...             NaN   \n",
      "1881          NaN               NaN               NaN  ...             NaN   \n",
      "1882          NaN               NaN               NaN  ...             NaN   \n",
      "1883          NaN               NaN               NaN  ...             NaN   \n",
      "\n",
      "      RCC_RMS_OFE  RCC_RMS_Other1  RCC_RMS_Other2  RCC_RMS_Other3  RCC_RMS_SF  \\\n",
      "0             NaN             NaN             NaN             NaN         NaN   \n",
      "1             NaN             NaN             NaN             NaN         NaN   \n",
      "2             NaN             NaN             NaN             NaN         NaN   \n",
      "3             NaN             NaN             NaN             NaN         NaN   \n",
      "4             NaN             NaN             NaN             NaN         NaN   \n",
      "...           ...             ...             ...             ...         ...   \n",
      "1879          NaN             NaN             NaN             NaN         NaN   \n",
      "1880          NaN             NaN             NaN             NaN         NaN   \n",
      "1881          NaN             NaN             NaN             NaN         NaN   \n",
      "1882          NaN             NaN             NaN             NaN         NaN   \n",
      "1883          NaN             NaN             NaN             NaN         NaN   \n",
      "\n",
      "      RCC_RMS_TLA  Round_Survey  T_ID  UC_ID  \n",
      "0             NaN           1.5   1.0    1.0  \n",
      "1             NaN           1.5   1.0    1.0  \n",
      "2             NaN           1.5   1.0    1.0  \n",
      "3             NaN           1.5   1.0    1.0  \n",
      "4             NaN           1.5   1.0    1.0  \n",
      "...           ...           ...   ...    ...  \n",
      "1879          NaN           1.5  47.0   73.0  \n",
      "1880          NaN           1.5  47.0   73.0  \n",
      "1881          NaN           1.5  47.0   73.0  \n",
      "1882          NaN           1.5  48.0   74.0  \n",
      "1883          NaN           1.5  48.0   74.0  \n",
      "\n",
      "[1884 rows x 86 columns]\n",
      "Merged data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# for a single unique identifier\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty DataFrame to hold the merged data\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each file in the list, read it, and concatenate it to the merged DataFrame\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(data_folder, file_name)  # Create the full file path\n",
    "    print(f\"Looking for file: {file_path}\")  # Debug print\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"File found: {file_path}\")  # Debug print\n",
    "        df = pd.read_csv(file_path, dtype=str)  # Read the CSV file with all columns as strings to handle mixed data types\n",
    "\n",
    "        # Convert numeric columns to double (if possible)\n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                df[col] = pd.to_numeric(df[col], errors='raise', downcast='float')\n",
    "            except ValueError:\n",
    "                # Keep non-convertible columns as strings\n",
    "                pass\n",
    "\n",
    "        merged_df = pd.concat([merged_df, df], ignore_index=True)  # Concatenate the current file's data to the merged DataFrame\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")  # Debug print\n",
    "\n",
    "# Proceed only if merged_df is not empty\n",
    "if not merged_df.empty:\n",
    "    # Define the merge keys\n",
    "    merge_keys = ['HID']\n",
    "    # Filter out merge keys that are not in the DataFrame\n",
    "    available_merge_keys = [key for key in merge_keys if key in merged_df.columns]\n",
    "\n",
    "    # Function to handle aggregation of non-merge key columns\n",
    "    def aggregate_data(group):\n",
    "        # Initialize an empty dictionary to hold aggregated results\n",
    "        agg_dict = {}\n",
    "        for col in group.columns:\n",
    "            if col in available_merge_keys:\n",
    "                # Keep merge keys as they are\n",
    "                agg_dict[col] = group[col].iloc[0]\n",
    "            else:\n",
    "                # For numeric columns, sum the values\n",
    "                if pd.api.types.is_numeric_dtype(group[col]):\n",
    "                    agg_dict[col] = group[col].sum()\n",
    "                else:\n",
    "                    # For string columns, concatenate all values\n",
    "                    agg_dict[col] = ', '.join(group[col].dropna().astype(str).unique())\n",
    "        return pd.Series(agg_dict)\n",
    "\n",
    "    # Check for initial duplicates\n",
    "    initial_duplicates = merged_df[merged_df.duplicated(subset=available_merge_keys, keep=False)]\n",
    "    if not initial_duplicates.empty:\n",
    "        print(f\"Initial duplicates found based on keys {available_merge_keys}:\")\n",
    "        print(initial_duplicates)\n",
    "\n",
    "    # Handle rows with missing keys separately\n",
    "    complete_keys_df = merged_df.dropna(subset=available_merge_keys)\n",
    "    incomplete_keys_df = merged_df[merged_df[available_merge_keys].isnull().any(axis=1)]\n",
    "\n",
    "    # Drop rows with missing merge keys\n",
    "    merged_df.dropna(subset=available_merge_keys, inplace=True)\n",
    "\n",
    "    # Apply aggregation function to each group for rows with complete keys\n",
    "    if not merged_df.empty:\n",
    "        merged_df = merged_df.groupby(available_merge_keys).apply(aggregate_data).reset_index(drop=True)\n",
    "\n",
    "    # Check for duplicates after aggregation\n",
    "    post_aggregation_duplicates = merged_df[merged_df.duplicated(subset=available_merge_keys, keep=False)]\n",
    "    if not post_aggregation_duplicates.empty:\n",
    "        print(f\"Post-aggregation duplicates found based on keys {available_merge_keys}:\")\n",
    "        print(post_aggregation_duplicates)\n",
    "\n",
    "    # Reorder columns to have available merge keys first and then the rest alphabetically\n",
    "    remaining_columns = [col for col in merged_df.columns if col not in available_merge_keys]  # Get columns excluding available merge keys\n",
    "    # Sort the remaining columns alphabetically; put columns starting with digits at the end\n",
    "    sorted_columns = sorted(remaining_columns, key=lambda x: (x[0].isdigit(), x))\n",
    "\n",
    "    # Define the final column order with available merge keys first\n",
    "    column_order = available_merge_keys + sorted_columns\n",
    "    # Reorder the DataFrame columns according to the defined order\n",
    "    merged_df = merged_df[column_order]\n",
    "\n",
    "    # Remove columns that are entirely empty\n",
    "    merged_df.dropna(axis=1, how='all', inplace=True)\n",
    "    # Remove rows that are entirely empty\n",
    "    merged_df.dropna(axis=0, how='all', inplace=True)\n",
    "\n",
    "    # Append rows with incomplete keys to the end of the DataFrame\n",
    "    final_df = pd.concat([merged_df, incomplete_keys_df], ignore_index=True)\n",
    "\n",
    "    # Save the merged and processed data to a new CSV file\n",
    "    final_df.to_csv('17. ClimateChange_Household-HouseholdMember_Level Updated.csv', index=False)\n",
    "    print(\"Merged data saved successfully!\")\n",
    "else:\n",
    "    print(\"No data to merge and save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66f4afb9-88ff-4058-afa9-f9a684782841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for file: C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\MaleMerge\\3. MERGED CSV Sections\\16. AccessToExtension_Household-HouseholdMember_Level.csv\n",
      "File found: C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\MaleMerge\\3. MERGED CSV Sections\\16. AccessToExtension_Household-HouseholdMember_Level.csv\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No group keys passed!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 57\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Apply aggregation function to each group for rows with complete keys\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m merged_df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m---> 57\u001b[0m     merged_df \u001b[38;5;241m=\u001b[39m merged_df\u001b[38;5;241m.\u001b[39mgroupby(available_merge_keys)\u001b[38;5;241m.\u001b[39mapply(aggregate_data)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Reorder columns to have available merge keys first and then the rest alphabetically\u001b[39;00m\n\u001b[0;32m     60\u001b[0m remaining_columns \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m merged_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m available_merge_keys]  \u001b[38;5;66;03m# Get columns excluding available merge keys\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:8869\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   8866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   8867\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 8869\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[0;32m   8870\u001b[0m     obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   8871\u001b[0m     keys\u001b[38;5;241m=\u001b[39mby,\n\u001b[0;32m   8872\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   8873\u001b[0m     level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   8874\u001b[0m     as_index\u001b[38;5;241m=\u001b[39mas_index,\n\u001b[0;32m   8875\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m   8876\u001b[0m     group_keys\u001b[38;5;241m=\u001b[39mgroup_keys,\n\u001b[0;32m   8877\u001b[0m     observed\u001b[38;5;241m=\u001b[39mobserved,\n\u001b[0;32m   8878\u001b[0m     dropna\u001b[38;5;241m=\u001b[39mdropna,\n\u001b[0;32m   8879\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1278\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1278\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m get_grouper(\n\u001b[0;32m   1279\u001b[0m         obj,\n\u001b[0;32m   1280\u001b[0m         keys,\n\u001b[0;32m   1281\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   1282\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   1283\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m   1284\u001b[0m         observed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default \u001b[38;5;28;01melse\u001b[39;00m observed,\n\u001b[0;32m   1285\u001b[0m         dropna\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna,\n\u001b[0;32m   1286\u001b[0m     )\n\u001b[0;32m   1288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   1289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:1037\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[0;32m   1034\u001b[0m     groupings\u001b[38;5;241m.\u001b[39mappend(ping)\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(groupings) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obj):\n\u001b[1;32m-> 1037\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo group keys passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(groupings) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1039\u001b[0m     groupings\u001b[38;5;241m.\u001b[39mappend(Grouping(Index([], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m\"\u001b[39m), np\u001b[38;5;241m.\u001b[39marray([], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp)))\n",
      "\u001b[1;31mValueError\u001b[0m: No group keys passed!"
     ]
    }
   ],
   "source": [
    "# Initialize an empty DataFrame to hold the merged data\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each file in the list, read it, and concatenate it to the merged DataFrame\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(data_folder, file_name)  # Create the full file path\n",
    "    print(f\"Looking for file: {file_path}\")  # Debug print\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"File found: {file_path}\")  # Debug print\n",
    "        df = pd.read_csv(file_path, dtype=str)  # Read the CSV file with all columns as strings to handle mixed data types\n",
    "\n",
    "        # Convert numeric columns to double (if possible)\n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                df[col] = pd.to_numeric(df[col], errors='raise', downcast='float')\n",
    "            except ValueError:\n",
    "                # Keep non-convertible columns as strings\n",
    "                pass\n",
    "\n",
    "        merged_df = pd.concat([merged_df, df], ignore_index=True)  # Concatenate the current file's data to the merged DataFrame\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")  # Debug print\n",
    "\n",
    "# Proceed only if merged_df is not empty\n",
    "if not merged_df.empty:\n",
    "    # Define the merge keys\n",
    "    merge_keys = ['HID', 'PID']\n",
    "    # Filter out merge keys that are not in the DataFrame\n",
    "    available_merge_keys = [key for key in merge_keys if key in merged_df.columns]\n",
    "\n",
    "    # Function to handle aggregation of non-merge key columns\n",
    "    def aggregate_data(group):\n",
    "        # Initialize an empty dictionary to hold aggregated results\n",
    "        agg_dict = {}\n",
    "        for col in group.columns:\n",
    "            if col in available_merge_keys:\n",
    "                # Keep merge keys as they are\n",
    "                agg_dict[col] = group[col].iloc[0]\n",
    "            else:\n",
    "                # For numeric columns, sum the values\n",
    "                if pd.api.types.is_numeric_dtype(group[col]):\n",
    "                    agg_dict[col] = group[col].sum()\n",
    "                else:\n",
    "                    # For string columns, concatenate all values\n",
    "                    agg_dict[col] = ', '.join(group[col].dropna().astype(str).unique())\n",
    "        return pd.Series(agg_dict)\n",
    "\n",
    "    # Handle rows with missing keys separately\n",
    "    complete_keys_df = merged_df.dropna(subset=available_merge_keys)\n",
    "    incomplete_keys_df = merged_df[merged_df[available_merge_keys].isnull().any(axis=1)]\n",
    "\n",
    "    # Drop rows with missing merge keys\n",
    "    merged_df.dropna(subset=available_merge_keys, inplace=True)\n",
    "\n",
    "    # Apply aggregation function to each group for rows with complete keys\n",
    "    if not merged_df.empty:\n",
    "        merged_df = merged_df.groupby(available_merge_keys).apply(aggregate_data).reset_index(drop=True)\n",
    "\n",
    "    # Reorder columns to have available merge keys first and then the rest alphabetically\n",
    "    remaining_columns = [col for col in merged_df.columns if col not in available_merge_keys]  # Get columns excluding available merge keys\n",
    "    # Sort the remaining columns alphabetically; put columns starting with digits at the end\n",
    "    sorted_columns = sorted(remaining_columns, key=lambda x: (x[0].isdigit(), x))\n",
    "\n",
    "    # Define the final column order with available merge keys first\n",
    "    column_order = available_merge_keys + sorted_columns\n",
    "    # Reorder the DataFrame columns according to the defined order\n",
    "    complete_keys_df = complete_keys_df[column_order]\n",
    "\n",
    "    # Remove columns that are entirely empty\n",
    "    complete_keys_df.dropna(axis=1, how='all', inplace=True)\n",
    "    # Remove rows that are entirely empty\n",
    "    complete_keys_df.dropna(axis=0, how='all', inplace=True)\n",
    "\n",
    "    # Append rows with incomplete keys to the end of the DataFrame\n",
    "    final_df = pd.concat([complete_keys_df, incomplete_keys_df], ignore_index=True)\n",
    "\n",
    "    # Save the merged and processed data to a new CSV file\n",
    "    final_df.to_csv('15. ExtendedFamily_Household-HouseholdMember_Level Updated.csv', index=False)\n",
    "    print(\"Merged data saved successfully!\")\n",
    "else:\n",
    "    print(\"No data to merge and save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c85776-2c14-4ae6-8a80-11526b9ff981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path and the keys to check\n",
    "file_path = r\"C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\MaleMerge\\3. MERGED CSV Sections\\3. Agriculture_Household-HouseholdMember_Level Updated.csv\"\n",
    "merge_keys = ['HID', 'PID']\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "# Check for duplicates based on the merge keys\n",
    "duplicates = df[df.duplicated(subset=merge_keys, keep=False)]\n",
    "\n",
    "# Print the results\n",
    "if not duplicates.empty:\n",
    "    print(f\"Found duplicates based on the keys {merge_keys}:\")\n",
    "    print(duplicates)\n",
    "    print(f\"\\nTotal number of duplicate rows: {duplicates.shape[0]}\")\n",
    "else:\n",
    "    print(f\"No duplicates found based on the keys {merge_keys}.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1b82ec6-0cfa-4609-812c-ae3f55d53c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for file: C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\MaleMerge\\3. MERGED CSV Sections\\3.1. Agriculture_Household-HouseholdMember-Plot-Crop-Season_Level.csv\n",
      "File found: C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\MaleMerge\\3. MERGED CSV Sections\\3.1. Agriculture_Household-HouseholdMember-Plot-Crop-Season_Level.csv\n",
      "Merged data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# HID, PID, PlotID, CropID, Season Level\n",
    "\n",
    "# Initialize an empty DataFrame to hold the merged data\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each file in the list, read it, and concatenate it to the merged DataFrame\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(data_folder, file_name)  # Create the full file path\n",
    "    print(f\"Looking for file: {file_path}\")  # Debug print\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"File found: {file_path}\")  # Debug print\n",
    "        df = pd.read_csv(file_path, dtype=str)  # Read the CSV file with all columns as strings to handle mixed data types\n",
    "\n",
    "        # Convert numeric columns to double (if possible)\n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                df[col] = pd.to_numeric(df[col], errors='raise', downcast='float')\n",
    "            except ValueError:\n",
    "                # Keep non-convertible columns as strings\n",
    "                pass\n",
    "\n",
    "        merged_df = pd.concat([merged_df, df], ignore_index=True)  # Concatenate the current file's data to the merged DataFrame\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")  # Debug print\n",
    "\n",
    "# Proceed only if merged_df is not empty\n",
    "if not merged_df.empty:\n",
    "    # Define the merge keys\n",
    "    merge_keys = ['HID', 'PID', 'Plt_ID', 'Crop_ID', 'Season']\n",
    "    # Filter out merge keys that are not in the DataFrame\n",
    "    available_merge_keys = [key for key in merge_keys if key in merged_df.columns]\n",
    "\n",
    "    # Function to handle aggregation of non-merge key columns\n",
    "    def aggregate_data(group):\n",
    "        # Initialize an empty dictionary to hold aggregated results\n",
    "        agg_dict = {}\n",
    "        for col in group.columns:\n",
    "            if col in available_merge_keys:\n",
    "                # Keep merge keys as they are\n",
    "                agg_dict[col] = group[col].iloc[0]\n",
    "            else:\n",
    "                # For numeric columns, sum the values\n",
    "                if pd.api.types.is_numeric_dtype(group[col]):\n",
    "                    agg_dict[col] = group[col].sum()\n",
    "                else:\n",
    "                    # For string columns, concatenate all values\n",
    "                    agg_dict[col] = ', '.join(group[col].dropna().astype(str).unique())\n",
    "        return pd.Series(agg_dict)\n",
    "\n",
    "    # Handle rows with missing keys separately\n",
    "    complete_keys_df = merged_df.dropna(subset=available_merge_keys)\n",
    "    incomplete_keys_df = merged_df[merged_df[available_merge_keys].isnull().any(axis=1)]\n",
    "\n",
    "    # Drop rows with missing merge keys\n",
    "    merged_df.dropna(subset=available_merge_keys, inplace=True)\n",
    "\n",
    "    # Apply aggregation function to each group for rows with complete keys\n",
    "    if not merged_df.empty:\n",
    "        merged_df = merged_df.groupby(available_merge_keys).apply(aggregate_data).reset_index(drop=True)\n",
    "\n",
    "    # Reorder columns to have available merge keys first and then the rest alphabetically\n",
    "    remaining_columns = [col for col in merged_df.columns if col not in available_merge_keys]  # Get columns excluding available merge keys\n",
    "    # Sort the remaining columns alphabetically; put columns starting with digits at the end\n",
    "    sorted_columns = sorted(remaining_columns, key=lambda x: (x[0].isdigit(), x))\n",
    "\n",
    "    # Define the final column order with available merge keys first\n",
    "    column_order = available_merge_keys + sorted_columns\n",
    "    # Reorder the DataFrame columns according to the defined order\n",
    "    complete_keys_df = complete_keys_df[column_order]\n",
    "\n",
    "    # Remove columns that are entirely empty\n",
    "    complete_keys_df.dropna(axis=1, how='all', inplace=True)\n",
    "    # Remove rows that are entirely empty\n",
    "    complete_keys_df.dropna(axis=0, how='all', inplace=True)\n",
    "\n",
    "    # Append rows with incomplete keys to the end of the DataFrame\n",
    "    final_df = pd.concat([complete_keys_df, incomplete_keys_df], ignore_index=True)\n",
    "\n",
    "    # Save the merged and processed data to a new CSV file\n",
    "    final_df.to_csv('3.1. Agriculture_Household-HouseholdMember-Plot-Crop-Season_Level Updated.csv', index=False)\n",
    "    print(\"Merged data saved successfully!\")\n",
    "else:\n",
    "    print(\"No data to merge and save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6becaa91-504f-42a7-8a5a-52871ed59d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for file: C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\MaleMerge\\3. MERGED CSV Sections\\8.1. EconomicEventsAndShocks_Household-Season-ShockType_Level.csv\n",
      "File found: C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\MaleMerge\\3. MERGED CSV Sections\\8.1. EconomicEventsAndShocks_Household-Season-ShockType_Level.csv\n",
      "Merged data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# HID, Season, ShockID Level\n",
    "\n",
    "# Initialize an empty DataFrame to hold the merged data\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each file in the list, read it, and concatenate it to the merged DataFrame\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(data_folder, file_name)  # Create the full file path\n",
    "    print(f\"Looking for file: {file_path}\")  # Debug print\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"File found: {file_path}\")  # Debug print\n",
    "        df = pd.read_csv(file_path, dtype=str)  # Read the CSV file with all columns as strings to handle mixed data types\n",
    "\n",
    "        # Convert numeric columns to double (if possible)\n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                df[col] = pd.to_numeric(df[col], errors='raise', downcast='float')\n",
    "            except ValueError:\n",
    "                # Keep non-convertible columns as strings\n",
    "                pass\n",
    "\n",
    "        merged_df = pd.concat([merged_df, df], ignore_index=True)  # Concatenate the current file's data to the merged DataFrame\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")  # Debug print\n",
    "\n",
    "# Proceed only if merged_df is not empty\n",
    "if not merged_df.empty:\n",
    "    # Define the merge keys\n",
    "    merge_keys = ['HID', 'Season','Shock_ID']\n",
    "    # Filter out merge keys that are not in the DataFrame\n",
    "    available_merge_keys = [key for key in merge_keys if key in merged_df.columns]\n",
    "\n",
    "    # Function to handle aggregation of non-merge key columns\n",
    "    def aggregate_data(group):\n",
    "        # Initialize an empty dictionary to hold aggregated results\n",
    "        agg_dict = {}\n",
    "        for col in group.columns:\n",
    "            if col in available_merge_keys:\n",
    "                # Keep merge keys as they are\n",
    "                agg_dict[col] = group[col].iloc[0]\n",
    "            else:\n",
    "                # For numeric columns, sum the values\n",
    "                if pd.api.types.is_numeric_dtype(group[col]):\n",
    "                    agg_dict[col] = group[col].sum()\n",
    "                else:\n",
    "                    # For string columns, concatenate all values\n",
    "                    agg_dict[col] = ', '.join(group[col].dropna().astype(str).unique())\n",
    "        return pd.Series(agg_dict)\n",
    "\n",
    "    # Handle rows with missing keys separately\n",
    "    complete_keys_df = merged_df.dropna(subset=available_merge_keys)\n",
    "    incomplete_keys_df = merged_df[merged_df[available_merge_keys].isnull().any(axis=1)]\n",
    "\n",
    "    # Drop rows with missing merge keys\n",
    "    merged_df.dropna(subset=available_merge_keys, inplace=True)\n",
    "\n",
    "    # Apply aggregation function to each group for rows with complete keys\n",
    "    if not merged_df.empty:\n",
    "        merged_df = merged_df.groupby(available_merge_keys).apply(aggregate_data).reset_index(drop=True)\n",
    "\n",
    "    # Reorder columns to have available merge keys first and then the rest alphabetically\n",
    "    remaining_columns = [col for col in merged_df.columns if col not in available_merge_keys]  # Get columns excluding available merge keys\n",
    "    # Sort the remaining columns alphabetically; put columns starting with digits at the end\n",
    "    sorted_columns = sorted(remaining_columns, key=lambda x: (x[0].isdigit(), x))\n",
    "\n",
    "    # Define the final column order with available merge keys first\n",
    "    column_order = available_merge_keys + sorted_columns\n",
    "    # Reorder the DataFrame columns according to the defined order\n",
    "    complete_keys_df = complete_keys_df[column_order]\n",
    "\n",
    "    # Remove columns that are entirely empty\n",
    "    complete_keys_df.dropna(axis=1, how='all', inplace=True)\n",
    "    # Remove rows that are entirely empty\n",
    "    complete_keys_df.dropna(axis=0, how='all', inplace=True)\n",
    "\n",
    "    # Append rows with incomplete keys to the end of the DataFrame\n",
    "    final_df = pd.concat([complete_keys_df, incomplete_keys_df], ignore_index=True)\n",
    "\n",
    "    # Save the merged and processed data to a new CSV file\n",
    "    final_df.to_csv('8.1. EconomicEventsAndShocks_Household-Season-ShockType_Level Updated.csv', index=False)\n",
    "    print(\"Merged data saved successfully!\")\n",
    "else:\n",
    "    print(\"No data to merge and save.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3082e06-14b5-4ada-b379-645680cbe54d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
