{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e877b5-ee61-4e99-a33b-dcb46ee3c1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This notebook contains the code to merge all male data across the years based on sections.\n",
    "There are datasets for 2012, 2012-1.5, 2013 and 2014\n",
    "The different sections that will be merged are as follows:\n",
    "**2012**\n",
    "1. Cover\n",
    "2. Roaster\n",
    "3. Section 1: Education (All men 18 and above)\n",
    "4. Section 2: Agriculture\n",
    "5. Section 3: Assets\n",
    "6. Section 4: Consumption and Expenditure\n",
    "7. Section 5: Credit\n",
    "8. Section 6: Employment and Income\n",
    "9. Section 7: Economic Events/Shocks\n",
    "10. Section 8: Community Participation and Social Network Membership\n",
    "\n",
    "**2013**\n",
    "1. Cover\n",
    "2. Roaster\n",
    "3. Section 1: Education: Males 19 years and older\n",
    "4. Section 2: Agriculture\n",
    "5. Section 3: Assets\n",
    "6. Section 4: Consumption and Expenditure\n",
    "7. Section 5: Credit\n",
    "8. Section 6: Employment and Income\n",
    "9. Section 7: Health\n",
    "10. Section 8: Political Participation and Governance\n",
    "\n",
    "**2014**\n",
    "1. Cover\n",
    "2. Roaster\n",
    "3. Section 1: Education: Males 19 years and older\n",
    "4. Section 2: Agriculture\n",
    "5. Section 3: Assets\n",
    "6. Section 4: Consumption and Expenditure\n",
    "7. Section 5: Credit\n",
    "8. Section 6: Employment and Income\n",
    "9. Section 7: Economic Events/Shocks‚Äù\n",
    "10. Section 8: Participation in Social Safety Net\n",
    "11. Section 9: Siblings\n",
    "12. Section 10: Transfers\n",
    "13. Section 11: Health and Nutrition\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddcc85c3-5e73-405d-b1e7-4a08cad121ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "All files in the respective folder will be converted to xlsx format for readability\n",
    "This will be done for all male files across the years\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Path where the .dta files are located\n",
    "folder_path = r'C:\\Users\\warra\\Downloads\\data\\data\\2014_data\\Male'\n",
    "\n",
    "# Get a list of all .dta files in the specified directory\n",
    "file_list = glob.glob(folder_path + '/*.dta')\n",
    "\n",
    "# Loop through the list of files\n",
    "for file in file_list:\n",
    "    # Read the .dta file into a pandas DataFrame\n",
    "    df = pd.read_stata(file, convert_categoricals=False)\n",
    "    \n",
    "    # Define the output file name by replacing .dta with .xlsx\n",
    "    output_file = file.replace('.dta', '.xlsx')\n",
    "    \n",
    "    # Write the DataFrame to an Excel file\n",
    "    df.to_excel(output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdedd007-e29c-4664-8c47-81875441e502",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following code block will merge all roaster files across the years.\n",
    "* First, we are going to read the respective files and store them as data frames\n",
    "* Next, we are going to define column mappings that I have already figured out via manual methods\n",
    "* Once the mappings are done per the set rules, we will see the new roaster dataset across the years 1.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b06248f4-d50e-4ac4-9a12-91213643d91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block stores file paths to variables to make the code neat\n",
    "# The stored variables are called in the read_excel function and stored as dataframes\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Store excel file locations to variables (change it as per your path to file)\n",
    "roaster_2012=r\"C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\MaleMerge\\1. roaster\\002_roster_2012.xlsx\"\n",
    "roaster_2012_5=r\"C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\MaleMerge\\1. roaster\\005_roster_2012.5.xlsx\"\n",
    "roaster_2013=r\"C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\MaleMerge\\1. roaster\\049_roster_2013.xlsx\"\n",
    "roaster_2014=r\"C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\MaleMerge\\1. roaster\\061_roster_2014.xlsx\"\n",
    "\n",
    "# Read excel files\n",
    "df_2012 = pd.read_excel(roaster_2012)\n",
    "df_2012_5 = pd.read_excel(roaster_2012_5)\n",
    "df_2013 = pd.read_excel(roaster_2013)\n",
    "df_2014 = pd.read_excel(roaster_2014)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7a81876-e34e-4771-b3fd-060be59fa3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code block will be used to standardize column names across the years to avoid discrepancies during the merging process.\n",
    "For example in the roaster data for 2013 rq21 and rq23 are not the same as rq21 and rq23 in 2014 data, but they have the same variable names. Hence, we decide to rename such columns beforehand\n",
    "We will add the updated name to the mapping dictionaries instead of the original names.\n",
    "\n",
    "'''\n",
    "\n",
    "# Rename columns in df_2012\n",
    "df_2012.rename(columns={\n",
    "    'pid' : 'r_pid', \n",
    "    'q2': 'rq2', \n",
    "    'age_years': 'rq3_age_year',\n",
    "    'age_months': 'rq3_age_month',\n",
    "    'q4': 'rq13',\n",
    "    'q5': 'rq15',\n",
    "    'q6': 'rq16',\n",
    "    'q7': 'rq17',\n",
    "    'q8': 'rq18',\n",
    "    'q9': 'rq19',\n",
    "    'q10': 'rq23',\n",
    "    'q12': 'rq21'\n",
    "}, inplace=True)\n",
    "\n",
    "# Rename columns in df_2012_5\n",
    "df_2012_5.rename(columns={\n",
    "    'Round' : 'round',\n",
    "    'PID1' : 'r_pid',\n",
    "    'q2': 'rq2',\n",
    "    'age_years' : 'rq3_age_year',\n",
    "    'age_months': 'rq3_age_month',\n",
    "    'q4': 'rq13',\n",
    "    'q5': 'rq15',\n",
    "    'q6': 'rq16',\n",
    "    'q7': 'rq17',\n",
    "    'q8': 'rq18',\n",
    "    'q9': 'rq19',\n",
    "    'q10': 'rq23',\n",
    "    'q12': 'rq21'\n",
    "}, inplace=True)\n",
    "\n",
    "# Rename columns in df_2013\n",
    "df_2013.rename(columns={\n",
    "    'rq3_years': 'rq3_age_year',\n",
    "    'rq3_month': 'rq3_age_month',\n",
    "    'rq14':'rq5',\n",
    "    'rq23':'rq6',\n",
    "    'rq14':'rq5',\n",
    "    'rq4':'rq13',\n",
    "    'rq4b':'rq14',\n",
    "    'rq5' : 'rq15',\n",
    "    'rq6':'rq16',\n",
    "    'rq7':'rq17',\n",
    "    'rq8' : 'rq18',\n",
    "    'rq9' : 'rq19',\n",
    "    'rq10' : 'rq20',\n",
    "    'rq11' : 'rq21',\n",
    "    'rq13' : 'rq23',\n",
    "    'rq15' : 'rq24',\n",
    "    'rq16_country' : 'rq25',\n",
    "    'rq16_province' : 'rq26',\n",
    "    'rq16_district' : 'rq27',\n",
    "    'rq17_b' : 'rq32',\n",
    "    'rq18_year' : 'rq34',\n",
    "    'rq18_month' : 'rq35',\n",
    "    'rq19_year' : 'rq36',\n",
    "    'rq19_month' : 'rq37',\n",
    "    'rq20' : 'rq38',\n",
    "    'rq22_country' : 'rq39',\n",
    "    'rq22_province' : 'rq40',\n",
    "    'rq22_district' : 'rq41',\n",
    "    'rq24' : 'rq43',\n",
    "    'rq25' : 'rq44',\n",
    "    'rq21': 'rq21_2013'\n",
    "}, inplace=True)\n",
    "\n",
    "# df_2014 doesn't need renaming as it is the reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee7fb3b4-574e-4f18-a759-92b1d39f205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated mappings\n",
    "mapping_2012 = [\n",
    "    'hid', 'round', 'r_pid', 'rq2', 'rq3_age_year', 'rq3_age_month', None, None, None,\n",
    "    None, None, None, None, None, None, 'rq13', None, 'rq15', 'rq16', 'rq17', 'rq18', 'rq19',\n",
    "    None, 'rq21', 'rq23', None, None, None, None, None, None, None, None, None, None,\n",
    "    None, None, None, None, None, None, None, None, 'PROVINCE_ID', 'DISTRICT_ID', 'TEHSIL_ID',\n",
    "    'UC_ID', 'MAUZA_ID', None, None\n",
    "]\n",
    "\n",
    "mapping_2012_5 = [\n",
    "    'hid', 'round', 'r_pid', 'rq2', 'rq3_age_year', 'rq3_age_month', None, None, None, None,\n",
    "    None, None, None, None, None, 'rq13', None, 'rq15', 'rq16', 'rq17', 'rq18', 'rq19', None,\n",
    "    'rq21', 'rq23', 'Q16_C_CODE', 'Q16_P_CODE', 'Q16_D_CODE', None, None, None, None, None,\n",
    "    None, None, None, None, None, None, None, None, 'C_PROVINCE', 'C_DISTRICT', 'C_TEHSIL',\n",
    "    'C_UC', 'C_MOUZA', 'C_HH_NUM', 'Q16_Village'\n",
    "]\n",
    "\n",
    "mapping_2013 = [\n",
    "    'hid', 'round', 'r_pid', 'rq2', 'rq3_age_year', 'rq3_age_month', None, 'rq5', 'rq6', None, None,\n",
    "    None, None, None, None, 'rq13', 'rq14', 'rq15', 'rq16', 'rq17', 'rq18', 'rq19', 'rq20', 'rq21_2013', 'rq23', 'rq24',\n",
    "    'rq25', 'rq26', 'rq27', 'rq32', 'rq34', 'rq35', 'rq36', 'rq37', 'rq38', 'rq39', 'rq40', 'rq41', 'rq43', 'rq44', None, None, None, None,\n",
    "    None, None, None, None, None\n",
    "]\n",
    "\n",
    "mapping_2014 = [\n",
    "    'hid', 'round', 'r_pid', 'rq2', 'rq3_age_year', 'rq3_age_month', 'rq4', 'rq5', 'rq6',\n",
    "    'rq7', 'rq8', 'rq9', 'rq10', 'rq11', 'rq12', 'rq13', 'rq14', 'rq15', 'rq16', 'rq17',\n",
    "    'rq18', 'rq19', 'rq20', 'rq21', 'rq23', 'rq24', 'rq25', 'rq26', 'rq27', 'rq32',\n",
    "    'rq34', 'rq35', 'rq36', 'rq37', 'rq38', 'rq39', 'rq40', 'rq41', 'rq43', 'rq44', None, None, None, None,\n",
    "    None, None, None, None, None\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e291578-5fa0-4a2d-abb6-1dfa95ced1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "    Standardize and merge dataframes based on reference mapping.\n",
    "\n",
    "    Parameters:\n",
    "    dfs (list of pd.DataFrame): List of dataframes to be merged.\n",
    "    mappings (list of list): List of mappings corresponding to each dataframe.\n",
    "    ref_mapping (list): Reference mapping to standardize the column names.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The merged dataframe with standardized column names.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "def standardize_and_merge(dfs, mappings, ref_mapping, df_names):\n",
    "    # Create a dictionary to store columns from all dataframes\n",
    "    merged_data = {col: [] for col in ref_mapping if col}\n",
    "    \n",
    "    max_len = 0  # To track the maximum length of columns\n",
    "\n",
    "    # Iterate through each dataframe and its corresponding mapping\n",
    "    for df, mapping, df_name in zip(dfs, mappings, df_names):\n",
    "        for i, col in enumerate(mapping):\n",
    "            if col and i < len(ref_mapping):\n",
    "                ref_col = ref_mapping[i]\n",
    "                if ref_col:  # Reference column is not None\n",
    "                    if col in df.columns:\n",
    "                        # If the reference column is in the merged_data, append the data\n",
    "                        if ref_col in merged_data:\n",
    "                            try:\n",
    "                                merged_data[ref_col].extend(df[col].tolist())\n",
    "                            except AttributeError as e:\n",
    "                                print(f\"Error with column {col} in dataframe {df_name}: {e}\")\n",
    "                        else:\n",
    "                            try:\n",
    "                                merged_data[ref_col] = df[col].tolist()\n",
    "                            except AttributeError as e:\n",
    "                                print(f\"Error with column {col} in dataframe {df_name}: {e}\")\n",
    "                        # Update max length of the columns\n",
    "                        max_len = max(max_len, len(merged_data[ref_col]))\n",
    "                elif col in df.columns:\n",
    "                    # For columns in the dataframes but not in the reference mapping\n",
    "                    new_col_name = f\"{df_name}_{col}\"\n",
    "                    if new_col_name not in merged_data:\n",
    "                        try:\n",
    "                            merged_data[new_col_name] = df[col].tolist()\n",
    "                        except AttributeError as e:\n",
    "                            print(f\"Error with column {col} in dataframe {df_name}: {e}\")\n",
    "                        # Update max length of the columns\n",
    "                        max_len = max(max_len, len(merged_data[new_col_name]))\n",
    "                    else:\n",
    "                        # If already present, extend the list\n",
    "                        try:\n",
    "                            merged_data[new_col_name].extend(df[col].tolist())\n",
    "                        except AttributeError as e:\n",
    "                            print(f\"Error with column {col} in dataframe {df_name}: {e}\")\n",
    "                        max_len = max(max_len, len(merged_data[new_col_name]))\n",
    "\n",
    "        # Add columns not in the mapping\n",
    "        for col in df.columns:\n",
    "            if col not in mapping:\n",
    "                new_col_name = f\"{df_name}_{col}\"\n",
    "                if new_col_name not in merged_data:\n",
    "                    merged_data[new_col_name] = df[col].tolist()\n",
    "                    max_len = max(max_len, len(merged_data[new_col_name]))\n",
    "                else:\n",
    "                    # If already present, extend the list\n",
    "                    merged_data[new_col_name].extend(df[col].tolist())\n",
    "                    max_len = max(max_len, len(merged_data[new_col_name]))\n",
    "\n",
    "    # Ensure all columns have the same length\n",
    "    for key in merged_data:\n",
    "        col_len = len(merged_data[key])\n",
    "        if col_len < max_len:\n",
    "            merged_data[key].extend([np.nan] * (max_len - col_len))\n",
    "\n",
    "    # Convert the merged_data dictionary to a DataFrame\n",
    "    merged_df = pd.DataFrame.from_dict(merged_data)\n",
    "    \n",
    "    # Remove columns containing 'Unnamed'\n",
    "    merged_df = merged_df.loc[:, ~merged_df.columns.str.contains('Unnamed')]\n",
    "\n",
    "    return merged_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7bdd3969-3848-486d-af74-f8970359b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_and_merge(dfs, mappings, ref_mapping, df_names):\n",
    "    \"\"\"\n",
    "    Standardize and merge dataframes based on reference mapping.\n",
    "\n",
    "    Parameters:\n",
    "    dfs (list of pd.DataFrame): List of dataframes to be merged.\n",
    "    mappings (list of list): List of mappings corresponding to each dataframe.\n",
    "    ref_mapping (list): Reference mapping to standardize the column names.\n",
    "    df_names (list): List of dataframe names.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The merged dataframe with standardized column names.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a dictionary to store columns from all dataframes\n",
    "    merged_data = {col: [] for col in ref_mapping if col}\n",
    "    \n",
    "    max_len = 0  # To track the maximum length of columns\n",
    "\n",
    "    # Iterate through each dataframe and its corresponding mapping\n",
    "    for df, mapping, df_name in zip(dfs, mappings, df_names):\n",
    "        for i, col in enumerate(mapping):\n",
    "            if col and i < len(ref_mapping):\n",
    "                ref_col = ref_mapping[i]\n",
    "                if ref_col:  # Reference column is not None\n",
    "                    if col in df.columns:\n",
    "                        # If the reference column is in the merged_data, append the data\n",
    "                        if ref_col in merged_data:\n",
    "                            merged_data[ref_col].extend(df[col].tolist())\n",
    "                        else:\n",
    "                            merged_data[ref_col] = df[col].tolist()\n",
    "                        # Update max length of the columns\n",
    "                        max_len = max(max_len, len(merged_data[ref_col]))\n",
    "                elif col in df.columns:\n",
    "                    # For columns in the dataframes but not in the reference mapping\n",
    "                    new_col_name = f\"{df_name}_{col}\"\n",
    "                    if new_col_name not in merged_data:\n",
    "                        merged_data[new_col_name] = df[col].tolist()\n",
    "                        # Update max length of the columns\n",
    "                        max_len = max(max_len, len(merged_data[new_col_name]))\n",
    "                    else:\n",
    "                        # If already present, extend the list\n",
    "                        merged_data[new_col_name].extend(df[col].tolist())\n",
    "                        max_len = max(max_len, len(merged_data[new_col_name]))\n",
    "            elif i < len(ref_mapping) and ref_mapping[i] is None:\n",
    "                # Add columns from 2012 mapped onto 2012_5\n",
    "                if df_name == 'df_2012':\n",
    "                    col_name = mapping[i]\n",
    "                    if col_name in df.columns:\n",
    "                        new_col_name = f\"{df_name}_{col_name}\"\n",
    "                        if new_col_name not in merged_data:\n",
    "                            merged_data[new_col_name] = df[col_name].tolist()\n",
    "                            max_len = max(max_len, len(merged_data[new_col_name]))\n",
    "                        else:\n",
    "                            merged_data[new_col_name].extend(df[col_name].tolist())\n",
    "                            max_len = max(max_len, len(merged_data[new_col_name]))\n",
    "\n",
    "    # Ensure all columns have the same length\n",
    "    for key in merged_data:\n",
    "        col_len = len(merged_data[key])\n",
    "        if col_len < max_len:\n",
    "            merged_data[key].extend([np.nan] * (max_len - col_len))\n",
    "\n",
    "    # Convert the merged_data dictionary to a DataFrame\n",
    "    merged_df = pd.DataFrame.from_dict(merged_data)\n",
    "    \n",
    "    # Remove columns containing 'Unnamed'\n",
    "    merged_df = merged_df.loc[:, ~merged_df.columns.str.contains('Unnamed')]\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3a9d73f8-f281-47d4-8a77-829a543089dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes and their mappings\n",
    "dfs = [df_2012, df_2012_5, df_2013, df_2014]\n",
    "mappings = [mapping_2012, mapping_2012_5, mapping_2013, mapping_2014]\n",
    "ref_mapping = mapping_2014\n",
    "df_names = ['df_2012', 'df_2012_5', 'df_2013', 'df_2014']\n",
    "\n",
    "# Merge the dataframes\n",
    "merged_df = standardize_and_merge(dfs, mappings, mapping_2014, df_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e3371dff-22c9-4ce4-abbd-c0f8010f344c",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'merged_roster.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save the merged dataframe to a CSV file\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m merged_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerged_roster.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:3902\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3891\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3893\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3894\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3895\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3899\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3900\u001b[0m )\n\u001b[1;32m-> 3902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m   3903\u001b[0m     path_or_buf,\n\u001b[0;32m   3904\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   3905\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   3906\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   3907\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   3908\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3909\u001b[0m     quoting\u001b[38;5;241m=\u001b[39mquoting,\n\u001b[0;32m   3910\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   3911\u001b[0m     index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[0;32m   3912\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   3913\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m   3914\u001b[0m     quotechar\u001b[38;5;241m=\u001b[39mquotechar,\n\u001b[0;32m   3915\u001b[0m     date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[0;32m   3916\u001b[0m     doublequote\u001b[38;5;241m=\u001b[39mdoublequote,\n\u001b[0;32m   3917\u001b[0m     escapechar\u001b[38;5;241m=\u001b[39mescapechar,\n\u001b[0;32m   3918\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   3919\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1152\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1134\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1135\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1151\u001b[0m )\n\u001b[1;32m-> 1152\u001b[0m csv_formatter\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1155\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:247\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    250\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    251\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,\n\u001b[0;32m    252\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression,\n\u001b[0;32m    253\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[0;32m    254\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    257\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    258\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    264\u001b[0m     )\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'merged_roster.csv'"
     ]
    }
   ],
   "source": [
    "# Save the merged dataframe to a CSV file\n",
    "merged_df.to_csv('merged_roster.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "51a5aad7-5898-4f02-948f-c3bcced01f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for the merged file (if needed)\n",
    "rename_mapping = {\n",
    "    'hid': 'HID',\n",
    "    'round': 'Survey_Round',\n",
    "    'r_pid': 'PID',\n",
    "    'rq2': 'Gender', \n",
    "    'rq3_age_year': 'Age_Years',\n",
    "    'rq3_age_month': 'Age_Months',\n",
    "    'rq4': 'Change_NAG',\n",
    "    'rq5': 'Status_Member',\n",
    "    'rq6': 'Confirm_Birth_Area',\n",
    "    'rq7': 'Reloc_year',\n",
    "    'rq8': 'C_Birth',\n",
    "    'rq9': 'P_Birth',\n",
    "    'rq8': 'D_Birth',\n",
    "    'rq10': 'T_Birth',\n",
    "    'rq11': 'M_Birth',\n",
    "    'rq12': 'Rel_HoH',\n",
    "    'rq13': 'Cur_Rel_HoH',\n",
    "    'rq14': 'Status_Marital',\n",
    "    'rq15': 'Status_Spouse_H',\n",
    "    'rq16': 'PID_Spouse',\n",
    "    'rq17': 'PID_Father',\n",
    "    'rq18': 'PID_Mother',\n",
    "    'rq19': 'Status_Emp',\n",
    "    'rq20': 'CNIC',\n",
    "    'rq21': 'Away_Months',\n",
    "    'rq23': 'Res_Not_Mem_H',\n",
    "    'rq24': 'Reloc_C',\n",
    "    'rq25': 'Reloc_P',\n",
    "    'rq26': 'Reloc_D',\n",
    "    'rq27': 'Rel_C_P',\n",
    "    'rq32': 'Year_Leave_H',\n",
    "    'rq34': 'Month_Leave_h',\n",
    "    'rq35': 'Year_Join_H',\n",
    "    'rq36': 'Month_Join_h',\n",
    "    'rq37': 'Res_Join_H',\n",
    "    'rq38': 'C_From',\n",
    "    'rq39': 'P_From',\n",
    "    'rq40': 'D_From',\n",
    "    'rq41': 'Year_Death',\n",
    "    'rq43': 'Res_Death',\n",
    "    'rq44': 'Away_Months_NewMem',\n",
    "    # Add other renaming mappings as needed\n",
    "}\n",
    "\n",
    "merged_df.rename(columns=rename_mapping, inplace=True)\n",
    "\n",
    "# Save the merged dataframe to a CSV file\n",
    "merged_df.to_csv('merged_roster.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f7835b-68d4-403c-a884-c36669ec385f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
