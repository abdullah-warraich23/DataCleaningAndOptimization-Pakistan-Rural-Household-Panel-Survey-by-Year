{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e877b5-ee61-4e99-a33b-dcb46ee3c1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This notebook contains the code to merge all male data across the years based on sections.\n",
    "There are datasets for 2012, 2012-1.5, 2013 and 2014\n",
    "The different sections that will be merged are as follows:\n",
    "**2012**\n",
    "1. Cover\n",
    "2. Roaster\n",
    "3. Section 1: Education (All men 18 and above)\n",
    "4. Section 2: Agriculture\n",
    "5. Section 3: Assets\n",
    "6. Section 4: Consumption and Expenditure\n",
    "7. Section 5: Credit\n",
    "8. Section 6: Employment and Income\n",
    "9. Section 7: Economic Events/Shocks\n",
    "10. Section 8: Community Participation and Social Network Membership\n",
    "\n",
    "**2013**\n",
    "1. Cover\n",
    "2. Roaster\n",
    "3. Section 1: Education: Males 19 years and older\n",
    "4. Section 2: Agriculture\n",
    "5. Section 3: Assets\n",
    "6. Section 4: Consumption and Expenditure\n",
    "7. Section 5: Credit\n",
    "8. Section 6: Employment and Income\n",
    "9. Section 7: Health\n",
    "10. Section 8: Political Participation and Governance\n",
    "\n",
    "**2014**\n",
    "1. Cover\n",
    "2. Roaster\n",
    "3. Section 1: Education: Males 19 years and older\n",
    "4. Section 2: Agriculture\n",
    "5. Section 3: Assets\n",
    "6. Section 4: Consumption and Expenditure\n",
    "7. Section 5: Credit\n",
    "8. Section 6: Employment and Income\n",
    "9. Section 7: Economic Events/Shocks‚Äù\n",
    "10. Section 8: Participation in Social Safety Net\n",
    "11. Section 9: Siblings\n",
    "12. Section 10: Transfers\n",
    "13. Section 11: Health and Nutrition\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc85c3-5e73-405d-b1e7-4a08cad121ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "All files in the male folder will be converted to xlsx format for readability\n",
    "This will be done for all male files across the years\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Path where the .dta files are located\n",
    "# We will use just once cell for each conversion since just the path is changing\n",
    "folder_path = r'C:\\Users\\warra\\Downloads\\data\\data\\2014_data\\Male'\n",
    "\n",
    "# Get a list of all .dta files in the specified directory\n",
    "file_list = glob.glob(folder_path + '/*.dta')\n",
    "\n",
    "# Loop through the list of files\n",
    "for file in file_list:\n",
    "    # Read the .dta file into a pandas DataFrame\n",
    "    df = pd.read_stata(file, convert_categoricals=False)\n",
    "    \n",
    "    # Define the output file name by replacing .dta with .xlsx\n",
    "    output_file = file.replace('.dta', '.xlsx')\n",
    "    \n",
    "    # Write the DataFrame to an Excel file\n",
    "    df.to_excel(output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdedd007-e29c-4664-8c47-81875441e502",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following code block will merge all roaster files across the years.\n",
    "* First, we are going to read the respective files and store them as data frames\n",
    "* Next, we are going to define column mappings that I have already figured out via manual methods\n",
    "* Once the mappings are done per the set rules, we will see the new roaster dataset across the years 1.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b06248f4-d50e-4ac4-9a12-91213643d91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block stores file paths to variables to make the code neat\n",
    "# The stored variables are called in the read_excel function and stored as dataframes\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Store excel file locations to variables (change it as per your path to file)\n",
    "agri_2012=r\"C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\MaleMerge\\3. Section 2 Part 1 OVERVIEW OF ALL AGRICLUTURE LAND\\2012_s2p1_m.xlsx\"\n",
    "agri_2013=r\"C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\MaleMerge\\3. Section 2 Part 1 OVERVIEW OF ALL AGRICLUTURE LAND\\2013_s2p1_m.xlsx\"\n",
    "agri_2014=r\"C:\\Users\\warra\\Desktop\\Freelance\\data\\data\\MaleMerge\\3. Section 2 Part 1 OVERVIEW OF ALL AGRICLUTURE LAND\\2014_s2p1_m.xlsx\"\n",
    "\n",
    "# Read excel files\n",
    "df_2012 = pd.read_excel(agri_2012)\n",
    "df_2013 = pd.read_excel(agri_2013)\n",
    "df_2014 = pd.read_excel(agri_2014)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7a81876-e34e-4771-b3fd-060be59fa3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code block will be used to standardize column names across the years to avoid discrepancies during the merging process.\n",
    "For example in the roaster data for 2013 rq21 and rq23 are not the same as rq21 and rq23 in 2014 data, but they have the same variable names. Hence, we decide to rename such columns beforehand\n",
    "We will add the updated name to the mapping dictionaries instead of the original names.\n",
    "\n",
    "'''\n",
    "\n",
    "# Rename columns in df_2012\n",
    "df_2012.rename(columns={\n",
    "    'land_own' : 's2p1_q1', \n",
    "    'land_sharein ': 's2p1_q2', \n",
    "    'land_rentin': 's2p1_q3',\n",
    "    'land_shareout': 's2p1_q4',\n",
    "    'land_rentout': 's2p1_q5',\n",
    "    'land_fallow': 's2p1_q6',\n",
    "    'land_cult': 's2p1_q7',\n",
    "    'season': 'Season',\n",
    "    'PROVINCE_ID' : 'P_ID',\n",
    "    'DISTRICT_ID' : 'D_ID',\n",
    "    'TEHSIL_ID' : 'T_ID',\n",
    "    'MAUZA_ID' : 'M_ID'\n",
    "    \n",
    "}, inplace=True)\n",
    "\n",
    "# Rename columns in df_2013\n",
    "df_2013.rename(columns={\n",
    "    's2p1_q1a' : 's2p1_q1',\n",
    "    's2p1_q2a' : 's2p1_q2',\n",
    "    's2p1_q4a' : 's2p1_q3',\n",
    "    's2p1_q3a' : 's2p1_q4',\n",
    "    's2p1_q5a' : 's2p1_q5',\n",
    "    's2p1_q6a' : 's2p1_q6',\n",
    "    's2p1_q7a' : 's2p1_q7',\n",
    "    'season' : 'Season',\n",
    "}, inplace=True)\n",
    "\n",
    "# df_2014 doesn't need renaming as it is the reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8ce41949-4ca3-48f4-abc7-a8adf6ad6fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the column mappings based on the provided positions\n",
    "# Make dictionaries for each year with updated names\n",
    "# replace with 'None' where there are no columns\n",
    "\n",
    "mapping_2012 = [\n",
    "    'hid', 'round', None, None, None, 's2p1_q1', 's2p1_q2', 's2p1_q3', 's2p1_q4', \n",
    "    's2p1_q5', 's2p1_q6', 's2p1_q7', None, 'Season', 'P_ID', 'D_ID', \n",
    "    'T_ID', 'UC_ID', 'M_ID'\n",
    "]\n",
    "\n",
    "mapping_2013 = [\n",
    "    'hid', 'round', 's2p1_qa', 's2p1_qb', None, 's2p1_q1', 's2p1_q2', 's2p1_q4', 's2p1_q3', \n",
    "    's2p1_q5', 's2p1_q6', 's2p1_q7', None, 'Season', None, None, None, None, None\n",
    "]\n",
    "\n",
    "mapping_2014 = [\n",
    "    'hid', 'round', 's2p1_qa', 's2p1_qb', 's2p1_sr_n', 's2p1_q1', 's2p1_q2', 's2p1_q3', 's2p1_q4', \n",
    "    's2p1_q5', 's2p1_q6', 's2p1_q7', 's2p1_q8', None, None, None, None, None, None\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b9ff0c33-1b53-44f3-8c0a-858987312571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all possible columns in the correct order\n",
    "all_columns = []\n",
    "for col in mapping_2012:\n",
    "    if col and col not in all_columns:\n",
    "        all_columns.append(col)\n",
    "for col in mapping_2013:\n",
    "    if col and col not in all_columns:\n",
    "        all_columns.append(col)\n",
    "for col in mapping_2014:\n",
    "    if col and col not in all_columns:\n",
    "        all_columns.append(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3136696-cf6d-41cf-9a84-98f82daa6c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_and_merge(dfs, mappings, ref_mapping, df_names):\n",
    "    \"\"\"\n",
    "    Standardize and merge dataframes based on reference mapping.\n",
    "\n",
    "    Parameters:\n",
    "    dfs (list of pd.DataFrame): List of dataframes to be merged.\n",
    "    mappings (list of list): List of mappings corresponding to each dataframe.\n",
    "    ref_mapping (list): Reference mapping to standardize the column names.\n",
    "    df_names (list): List of dataframe names.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The merged dataframe with standardized column names.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a dictionary to store columns from all dataframes\n",
    "    merged_data = {col: [] for col in ref_mapping if col}\n",
    "    max_len = 0  # To track the maximum length of columns\n",
    "\n",
    "    # Collect all unique columns from all dataframes\n",
    "    all_columns = set()\n",
    "    for df in dfs:\n",
    "        all_columns.update(df.columns)\n",
    "    \n",
    "    # Iterate through each dataframe and its corresponding mapping\n",
    "    for df, mapping, df_name in zip(dfs, mappings, df_names):\n",
    "        for i, col in enumerate(mapping):\n",
    "            if col and i < len(ref_mapping):\n",
    "                ref_col = ref_mapping[i]\n",
    "                if ref_col:  # Reference column is not None\n",
    "                    if col in df.columns:\n",
    "                        # If the reference column is in the merged_data, append the data\n",
    "                        if ref_col in merged_data:\n",
    "                            merged_data[ref_col].extend(df[col].tolist())\n",
    "                        else:\n",
    "                            merged_data[ref_col] = df[col].tolist()\n",
    "                        # Update max length of the columns\n",
    "                        max_len = max(max_len, len(merged_data[ref_col]))\n",
    "                elif col in df.columns:\n",
    "                    # For columns in the dataframes but not in the reference mapping\n",
    "                    new_col_name = f\"{df_name}_{col}\"\n",
    "                    if new_col_name not in merged_data:\n",
    "                        merged_data[new_col_name] = df[col].tolist()\n",
    "                        # Update max length of the columns\n",
    "                        max_len = max(max_len, len(merged_data[new_col_name]))\n",
    "                    else:\n",
    "                        # If already present, extend the list\n",
    "                        merged_data[new_col_name].extend(df[col].tolist())\n",
    "                        max_len = max(max_len, len(merged_data[new_col_name]))\n",
    "\n",
    "    # Include remaining columns that are not part of the reference mapping\n",
    "    for col in all_columns:\n",
    "        if col not in merged_data:\n",
    "            merged_data[col] = []\n",
    "            for df in dfs:\n",
    "                if col in df.columns:\n",
    "                    merged_data[col].extend(df[col].tolist())\n",
    "                    max_len = max(max_len, len(merged_data[col]))\n",
    "\n",
    "    # Ensure all columns have the same length\n",
    "    for key in merged_data:\n",
    "        col_len = len(merged_data[key])\n",
    "        if col_len < max_len:\n",
    "            merged_data[key].extend([np.nan] * (max_len - col_len))\n",
    "\n",
    "    # Convert the merged_data dictionary to a DataFrame\n",
    "    merged_df = pd.DataFrame.from_dict(merged_data)\n",
    "    \n",
    "    # Remove columns containing 'Unnamed'\n",
    "    merged_df = merged_df.loc[:, ~merged_df.columns.str.contains('Unnamed')]\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e4e582b-488e-438a-96d1-7389db12fed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes and their mappings\n",
    "dfs = [df_2012, df_2013, df_2014]\n",
    "mappings = [mapping_2012, mapping_2013, mapping_2014]\n",
    "ref_mapping = mapping_2014\n",
    "df_name= ['2012', '2013', '2014']\n",
    "merged_df = standardize_and_merge(dfs, mappings, ref_mapping, df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "787e0fcf-f875-4829-97c8-4a71ba2eecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_and_merge(dfs, mappings, all_columns):\n",
    "    merged_data = {col: [] for col in all_columns}\n",
    "\n",
    "    for df, mapping in zip(dfs, mappings):\n",
    "        for i, col in enumerate(mapping):\n",
    "            if col:\n",
    "                if col in df.columns:\n",
    "                    merged_data[col].extend(df[col].tolist())\n",
    "                else:\n",
    "                    merged_data[col].extend([np.nan] * len(df))\n",
    "    \n",
    "    max_len = max(len(v) for v in merged_data.values())\n",
    "    for key in merged_data:\n",
    "        col_len = len(merged_data[key])\n",
    "        if col_len < max_len:\n",
    "            merged_data[key].extend([np.nan] * (max_len - col_len))\n",
    "\n",
    "    merged_df = pd.DataFrame.from_dict(merged_data)\n",
    "    merged_df = merged_df.loc[:, ~merged_df.columns.str.contains('Unnamed')]\n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ca588b9-af97-48e7-8196-46949d4fb85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage with dataframes and mappings\n",
    "dfs = [df_2012, df_2013, df_2014]\n",
    "mappings = [mapping_2012, mapping_2013, mapping_2014]\n",
    "\n",
    "merged_df = standardize_and_merge(dfs, mappings, all_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51a5aad7-5898-4f02-948f-c3bcced01f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for the merged file (if needed)\n",
    "rename_mapping = {\n",
    "    'hid': 'HID',\n",
    "    'round': 'Round',\n",
    "    's2p1_qa': 'YN_Land_owned',\n",
    "    's2p1_qb': 'YN_Land_Cultiv',\n",
    "    's2p1_q1': 'Land_Owned',\n",
    "    's2p1_q2': 'Land_ShareIn',\n",
    "    's2p1_q3': 'Land_RentIn',\n",
    "    's2p1_q4': 'Land_ShareOut',\n",
    "    's2p1_q5': 'Land_RentOut',\n",
    "    's2p1_q6': 'Land_Fallow',\n",
    "    's2p1_q7': 'Land_Cult',\n",
    "    'P_ID': 'P_ID',\n",
    "    'D_ID': 'D_ID',\n",
    "    'T_ID': 'T_ID',\n",
    "    'UC_ID': 'UC_ID',\n",
    "    'M_ID': 'M_ID'\n",
    "}\n",
    "\n",
    "merged_df.rename(columns=rename_mapping, inplace=True)\n",
    "\n",
    "# Drop 's2p1_sr_n' if it's redundant\n",
    "if 's2p1_sr_n' in merged_df.columns:\n",
    "    merged_df.drop(columns=['s2p1_sr_n'], inplace=True)\n",
    "\n",
    "\n",
    "# Save the merged dataframe to a CSV file\n",
    "merged_df.to_csv('merged_Section_2_part_1.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a984fb-cef4-4d9e-9ac9-b680d6f41940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
